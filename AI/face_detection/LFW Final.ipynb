{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (2.11.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (4.7.0.68)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7.0)\n",
      "Requirement already satisfied: keras in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.1.21)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.30.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.24.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow opencv-python matplotlib keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard dependencies\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, Activation, Reshape\n",
    "from keras.layers import Concatenate, ZeroPadding2D, GlobalAveragePooling2D, Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first run this script for this images:\n",
    "IMAGES_DIR = 'data/WIDERFace/WIDER_train/images/'\n",
    "BOXES_PATH = 'data/WIDERFace/wider_face_split/wider_face_train_bbx_gt.txt'\n",
    "RESULT_DIR = 'data/WIDERFace/train/'\n",
    "\n",
    "# then run for this images:\n",
    "# IMAGES_DIR = 'data/WIDERFace/WIDER_val/images/'\n",
    "# BOXES_PATH = 'data/WIDERFace/wider_face_split/wider_val_train_bbx_gt.txt'\n",
    "# RESULT_DIR = 'data/WIDERFace/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 1e-4\n",
    "score_threshold = 0.05\n",
    "iou_threshold = 0.3\n",
    "image_size = 300\n",
    "\n",
    "localization_loss_weight = 1.0\n",
    "confidence_loss_weight = 1.0\n",
    "class_loss_weight = 1.0\n",
    "regularisation_loss_weight = 1e-3\n",
    "\n",
    "lr_bounds = [16000, 20000]\n",
    "lr_values = [4e-3, 4e-4, 4e-5]\n",
    "\n",
    "image_size = [1024, 1024]\n",
    "image_size = [300, 300]\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "num_classes = 2\n",
    "num_steps = 24000\n",
    "num_eval_steps = 1000\n",
    "train_dataset = \"data/WIDERFace/WIDER_train/\"\n",
    "val_dataset = \"data/WIDERFace/WIDER_val/\"\n",
    "train_annotations = \"data/WIDERFace/wider_face_split/wider_face_train_bbx_gt.txt\"\n",
    "val_annotations = \"data/WIDERFace/wider_face_split/wider_face_val_bbx_gt.txt\"\n",
    "train_tfrecord = \"data/WIDERFace/train.tfrecord\"\n",
    "val_tfrecord = \"data/WIDERFace/val.tfrecord\"\n",
    "checkpoint_dir = \"data/WIDERFace/checkpoints/\"\n",
    "log_dir = \"data/WIDERFace/logs/\"\n",
    "output_dir = \"data/WIDERFace/output/\"\n",
    "# Create output directory\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "# Create checkpoint directory\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Create log directory\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:00, 1016.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# collect paths to all images\n",
    "\n",
    "all_paths = []\n",
    "for path, subdirs, files in tqdm(os.walk(IMAGES_DIR)):\n",
    "    for name in files:\n",
    "        all_paths.append(os.path.join(path, name))\n",
    "        \n",
    "metadata = pd.DataFrame(all_paths, columns=['full_path'])\n",
    "\n",
    "# strip root folder\n",
    "metadata['path'] = metadata.full_path.apply(lambda x: os.path.relpath(x, IMAGES_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all unique endings\n",
    "metadata.path.apply(lambda x: x.split('.')[-1]).unique()\n",
    "\n",
    "# get all images\n",
    "len(metadata)\n",
    "\n",
    "# read all boxes\n",
    "with open(BOXES_PATH, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    content = [s.strip() for s in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'data\\\\WIDERFace\\\\WIDER_train\\\\images/0--Parade/0_Parade_marchingband_1_849.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     xmin, ymin, w, h \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)[:\u001b[39m4\u001b[39m]\n\u001b[1;32m---> 19\u001b[0m     xmin, ymin, w, h \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(xmin), \u001b[39mint\u001b[39m(ymin), \u001b[39mint\u001b[39m(w), \u001b[39mint\u001b[39m(h)\n\u001b[0;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m h \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m w \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     21\u001b[0m         \u001b[39mprint\u001b[39m(name)  \n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'data\\\\WIDERFace\\\\WIDER_train\\\\images/0--Parade/0_Parade_marchingband_1_849.jpg'"
     ]
    }
   ],
   "source": [
    "# split annotations by image\n",
    "boxes = {}\n",
    "num_lines = len(content)\n",
    "i = 0\n",
    "name = None\n",
    "\n",
    "while i < num_lines:\n",
    "    s = content[i]\n",
    "    file = s.split('/')[-1]\n",
    "    if s.endswith('.jpg'):\n",
    "        if name is not None:\n",
    "            assert len(boxes[name]) == num_boxes\n",
    "        name = s\n",
    "        boxes[name] = []\n",
    "        i += 1\n",
    "        num_boxes = int(content[i])\n",
    "        i += 1\n",
    "    else:\n",
    "        xmin, ymin, w, h = s.split(' ')[:4]\n",
    "        xmin, ymin, w, h = int(xmin), int(ymin), int(w), int(h)\n",
    "        if h <= 0 or w <= 0:\n",
    "            print(name)  \n",
    "            # some boxes are weird!\n",
    "            # so i don't use them\n",
    "            num_boxes -= 1\n",
    "        else:\n",
    "            boxes[name].append((xmin, ymin, w, h))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes_on_image(path, boxes):\n",
    "\n",
    "    image = Image.open(path)\n",
    "    draw = ImageDraw.Draw(image, 'RGBA')\n",
    "    width, height = image.size\n",
    "\n",
    "    for b in boxes:\n",
    "        xmin, ymin, w, h = b\n",
    "        xmax, ymax = xmin + w, ymin + h\n",
    "\n",
    "        fill = (255, 255, 255, 45)\n",
    "        outline = 'red'\n",
    "        draw.rectangle(\n",
    "            [(xmin, ymin), (xmax, ymax)],\n",
    "            fill=fill, outline=outline\n",
    "        )\n",
    "    return image\n",
    "\n",
    "i = random.randint(0, len(metadata) - 1)  # choose a random image\n",
    "some_boxes = boxes[metadata.path[i]]\n",
    "draw_boxes_on_image(metadata.full_path[i], some_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation(path, width, height):\n",
    "    name = path.split('/')[-1]\n",
    "    annotation = {\n",
    "      \"filename\": name,\n",
    "      \"size\": {\"depth\": 3, \"width\": width, \"height\": height}\n",
    "    }\n",
    "    objects = []\n",
    "    for b in boxes[path]:\n",
    "        xmin, ymin, w, h = b\n",
    "        xmax, ymax = xmin + w, ymin + h\n",
    "        objects.append({\n",
    "            \"bndbox\": {\"ymin\": ymin, \"ymax\": ymax, \"xmax\": xmax, \"xmin\": xmin}, \n",
    "            \"name\": \"face\"\n",
    "        })\n",
    "    annotation[\"object\"] = objects\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder for the converted dataset\n",
    "shutil.rmtree(RESULT_DIR, ignore_errors=True)\n",
    "os.mkdir(RESULT_DIR)\n",
    "os.mkdir(os.path.join(RESULT_DIR, 'images'))\n",
    "os.mkdir(os.path.join(RESULT_DIR, 'annotations'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for T in tqdm(metadata.itertuples()):\n",
    "    \n",
    "    # get width and height of an image\n",
    "    image = cv2.imread(T.full_path)\n",
    "    h, w, c = image.shape\n",
    "    assert c == 3\n",
    "    \n",
    "    # name of the image\n",
    "    name = T.path.split('/')[-1]\n",
    "    assert name.endswith('.jpg')\n",
    "\n",
    "    # copy the image\n",
    "    shutil.copy(T.full_path, os.path.join(RESULT_DIR, 'images', name))\n",
    "    \n",
    "    # save annotation for it\n",
    "    d = get_annotation(T.path, w, h)\n",
    "    json_name = name[:-4] + '.json'\n",
    "    json.dump(d, open(os.path.join(RESULT_DIR, 'annotations', json_name), 'w')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import math\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The purpose of this script is to create a set of .tfrecords files\n",
    "from a folder of images and a folder of annotations.\n",
    "Annotations are in the json format.\n",
    "Images must have .jpg or .jpeg filename extension.\n",
    "Example of a json annotation (with filename \"132416.json\"):\n",
    "{\n",
    "  \"object\": [\n",
    "    {\"bndbox\": {\"ymin\": 20, \"ymax\": 276, \"xmax\": 1219, \"xmin\": 1131}, \"name\": \"face\"},\n",
    "    {\"bndbox\": {\"ymin\": 1, \"ymax\": 248, \"xmax\": 1149, \"xmin\": 1014}, \"name\": \"face\"}\n",
    "  ],\n",
    "  \"filename\": \"132416.jpg\",\n",
    "  \"size\": {\"depth\": 3, \"width\": 1920, \"height\": 1080}\n",
    "}\n",
    "Example of use:\n",
    "python create_tfrecords.py \\\n",
    "    --image_dir=/home/gpu2/hdd/dan/WIDER/val/images/ \\\n",
    "    --annotations_dir=/home/gpu2/hdd/dan/WIDER/val/annotations/ \\\n",
    "    --output=data/train_shards/ \\\n",
    "    --num_shards=100\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def make_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-i', '--image_dir', type=str)\n",
    "    parser.add_argument('-a', '--annotations_dir', type=str)\n",
    "    parser.add_argument('-o', '--output', type=str)\n",
    "    parser.add_argument('-s', '--num_shards', type=int, default=1)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def dict_to_tf_example(annotation, image_dir):\n",
    "    \"\"\"Convert dict to tf.Example proto.\n",
    "    Notice that this function normalizes the bounding\n",
    "    box coordinates provided by the raw data.\n",
    "    Arguments:\n",
    "        data: a dict.\n",
    "        image_dir: a string, path to the image directory.\n",
    "    Returns:\n",
    "        an instance of tf.Example.\n",
    "    \"\"\"\n",
    "    image_name = annotation['filename']\n",
    "    assert image_name.endswith('.jpg') or image_name.endswith('.jpeg')\n",
    "\n",
    "    image_path = os.path.join(image_dir, image_name)\n",
    "    with tf.gfile.GFile(image_path, 'rb') as f:\n",
    "        encoded_jpg = f.read()\n",
    "\n",
    "    # check image format\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = PIL.Image.open(encoded_jpg_io)\n",
    "    if image.format != 'JPEG':\n",
    "        raise ValueError('Image format not JPEG!')\n",
    "\n",
    "    width = int(annotation['size']['width'])\n",
    "    height = int(annotation['size']['height'])\n",
    "    assert width > 0 and height > 0\n",
    "    assert image.size[0] == width and image.size[1] == height\n",
    "    ymin, xmin, ymax, xmax = [], [], [], []\n",
    "\n",
    "    just_name = image_name[:-4] if image_name.endswith('.jpg') else image_name[:-5]\n",
    "    annotation_name = just_name + '.json'\n",
    "    if len(annotation['object']) == 0:\n",
    "        print(annotation_name, 'is without any objects!')\n",
    "\n",
    "    for obj in annotation['object']:\n",
    "        a = float(obj['bndbox']['ymin'])/height\n",
    "        b = float(obj['bndbox']['xmin'])/width\n",
    "        c = float(obj['bndbox']['ymax'])/height\n",
    "        d = float(obj['bndbox']['xmax'])/width\n",
    "        assert (a < c) and (b < d)\n",
    "        ymin.append(a)\n",
    "        xmin.append(b)\n",
    "        ymax.append(c)\n",
    "        xmax.append(d)\n",
    "        assert obj['name'] == 'face'\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'filename': _bytes_feature(image_name.encode()),\n",
    "        'image': _bytes_feature(encoded_jpg),\n",
    "        'xmin': _float_list_feature(xmin),\n",
    "        'xmax': _float_list_feature(xmax),\n",
    "        'ymin': _float_list_feature(ymin),\n",
    "        'ymax': _float_list_feature(ymax),\n",
    "    }))\n",
    "    return example\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_list_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def main():\n",
    "    ARGS = make_args()\n",
    "\n",
    "    image_dir = ARGS.image_dir\n",
    "    annotations_dir = ARGS.annotations_dir\n",
    "    print('Reading images from:', image_dir)\n",
    "    print('Reading annotations from:', annotations_dir, '\\n')\n",
    "\n",
    "    examples_list = os.listdir(annotations_dir)\n",
    "    num_examples = len(examples_list)\n",
    "    print('Number of images:', num_examples)\n",
    "\n",
    "    num_shards = ARGS.num_shards\n",
    "    shard_size = math.ceil(num_examples/num_shards)\n",
    "    print('Number of images per shard:', shard_size)\n",
    "\n",
    "    output_dir = ARGS.output\n",
    "    shutil.rmtree(output_dir, ignore_errors=True)\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "    shard_id = 0\n",
    "    num_examples_written = 0\n",
    "    for example in tqdm(examples_list):\n",
    "\n",
    "        if num_examples_written == 0:\n",
    "            shard_path = os.path.join(output_dir, 'shard-%04d.tfrecords' % shard_id)\n",
    "            writer = tf.python_io.TFRecordWriter(shard_path)\n",
    "\n",
    "        path = os.path.join(annotations_dir, example)\n",
    "        annotation = json.load(open(path))\n",
    "        tf_example = dict_to_tf_example(annotation, image_dir)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "        num_examples_written += 1\n",
    "\n",
    "        if num_examples_written == shard_size:\n",
    "            shard_id += 1\n",
    "            num_examples_written = 0\n",
    "            writer.close()\n",
    "\n",
    "    if num_examples_written != shard_size and num_examples % num_shards != 0:\n",
    "        writer.close()\n",
    "\n",
    "    print('Result is here:', ARGS.output)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    \n",
    "    # Read in image from file path\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    # Load in the image \n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    \n",
    "    # Preprocessing steps - resizing the image to be 100x100x3\n",
    "    img = tf.image.resize(img, (100,100))\n",
    "    # Scale image to be between 0 and 1 \n",
    "    img = img / 255.0\n",
    "\n",
    "    # Return image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(img):\n",
    "    data = []\n",
    "    for i in range(9):\n",
    "        img = tf.image.stateless_random_brightness(img, max_delta=0.02, seed=(1,2))\n",
    "        img = tf.image.stateless_random_contrast(img, lower=0.6, upper=1, seed=(1,3))\n",
    "        # img = tf.image.stateless_random_crop(img, size=(20,20,3), seed=(1,2))\n",
    "        img = tf.image.stateless_random_flip_left_right(img, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_jpeg_quality(img, min_jpeg_quality=90, max_jpeg_quality=100, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_saturation(img, lower=0.9,upper=1, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "            \n",
    "        data.append(img)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SSD(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SSD, self).__init__()\n",
    "        \n",
    "        # Define the base VGG16 network\n",
    "        self.base = nn.ModuleList([\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ])\n",
    "        \n",
    "        # Define the auxiliary convolutional layers\n",
    "        self.aux_convs = nn.ModuleList([\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(512, 128, kernel_size=1),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(256, 128, kernel_size=1),\n",
    "            nn.Conv2d(128, 256, kernel_size=3),\n",
    "            nn.Conv2d(256, 128, kernel_size=1),\n",
    "            nn.Conv2d(128, 256, kernel_size=3)\n",
    "        ])\n",
    "        \n",
    "        # Define the localization and classification layers\n",
    "        self.loc_layers = nn.ModuleList([\n",
    "            nn.Conv2d(512, 4 * 4, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(1024, 6 * 4, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(512, 6 * 4, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(256, 6 * 4, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(256, 4 * 4, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(256, 4 * 4, kernel_size=3, padding=0),\n",
    "            nn.Conv2d(256, 4 * 4, kernel_size=3, padding=0),\n",
    "            nn.Conv2d(256, 4 * 4, kernel_size=3, padding=0)\n",
    "        ])\n",
    "        \n",
    "        self.cls_layers = nn.ModuleList([\n",
    "            nn.Conv2d(512, 4 * num_classes, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(1024, 6 * num_classes, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(512, 6 * num_classes, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(256, 6 * num_classes, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(256, 4 * num_classes, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(256, 4 * num_classes, kernel_size=3, padding=0),\n",
    "            nn.Conv2d(256, 4 * num_classes, kernel_size=3, padding=0),\n",
    "            nn.Conv2d(256, 4 * num_classes, kernel_size=3, padding=0)\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the SSD class\n",
    "ssd = SSD(num_classes=2)\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(300, 300, 3))\n",
    "\n",
    "# Define the base VGG16 network\n",
    "vgg16 = ssd.base(input_layer)\n",
    "\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = ssd(inputs=input_layer)\n",
    "\n",
    "# Compile the model with L2 regularization loss\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              loss_weights=[1., 0.01]) # set the weight for the L2 regularization loss\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Confidence loss\n",
    "def confidence_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the binary cross-entropy loss for the confidence scores.\n",
    "    y_true: true class labels (batch_size, num_anchors, num_classes+1)\n",
    "    y_pred: predicted class scores (batch_size, num_anchors, num_classes+1)\n",
    "    \"\"\"\n",
    "    # Extract the true and predicted class scores\n",
    "    true_scores = y_true[:,:,1:]\n",
    "    pred_scores = y_pred[:,:,1:]\n",
    "    \n",
    "    # Compute the binary cross-entropy loss\n",
    "    conf_loss = tf.keras.losses.binary_crossentropy(true_scores, pred_scores, from_logits=True)\n",
    "    \n",
    "    # Mask out the negative anchor boxes\n",
    "    mask = y_true[:,:,0]\n",
    "    conf_loss = conf_loss * mask\n",
    "    \n",
    "    # Compute the average loss over positive samples\n",
    "    num_positives = tf.reduce_sum(mask)\n",
    "    conf_loss = tf.reduce_sum(conf_loss) / (num_positives + 1e-6)\n",
    "    \n",
    "    return conf_loss\n",
    "\n",
    "# Localization loss\n",
    "def localization_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the smooth L1 loss for the bounding box locations.\n",
    "    y_true: true bounding box coordinates (batch_size, num_anchors, 4)\n",
    "    y_pred: predicted bounding box coordinates (batch_size, num_anchors, 4)\n",
    "    \"\"\"\n",
    "    # Compute the smooth L1 loss\n",
    "    loc_loss = tf.keras.losses.Huber(delta=1.0)(y_true, y_pred)\n",
    "    \n",
    "    # Mask out the negative anchor boxes\n",
    "    mask = tf.reduce_sum(tf.abs(y_true), axis=-1)\n",
    "    loc_loss = loc_loss * mask\n",
    "    \n",
    "    # Compute the average loss over positive samples\n",
    "    num_positives = tf.reduce_sum(mask)\n",
    "    loc_loss = tf.reduce_sum(loc_loss) / (num_positives + 1e-6)\n",
    "    \n",
    "    return loc_loss\n",
    "\n",
    "# Hard negative mining loss\n",
    "def hard_negative_mining_loss(y_true, y_pred, neg_ratio=3):\n",
    "    \"\"\"\n",
    "    Computes the hard negative mining loss for the negative anchor boxes.\n",
    "    y_true: true class labels (batch_size, num_anchors, num_classes+1)\n",
    "    y_pred: predicted class scores (batch_size, num_anchors, num_classes+1)\n",
    "    neg_ratio: the ratio of negative samples to positive samples\n",
    "    \"\"\"\n",
    "    # Extract the true and predicted class scores\n",
    "    true_scores = y_true[:,:,1:]\n",
    "    pred_scores = y_pred[:,:,1:]\n",
    "    \n",
    "    # Compute the binary cross-entropy loss\n",
    "    conf_loss = tf.keras.losses.binary_crossentropy(true_scores, pred_scores, from_logits=True)\n",
    "    \n",
    "    # Mask out the positive and negative anchor boxes\n",
    "    pos_mask = y_true[:,:,0]\n",
    "    neg_mask = tf.logical_not(pos_mask)\n",
    "    \n",
    "    # Compute the number of negative samples to keep\n",
    "    num_positives = tf.reduce_sum(pos_mask, axis=-1)\n",
    "    num_negatives = tf.reduce_sum(neg_mask, axis=-1)\n",
    "    num_negatives_keep = tf.cast(neg_ratio * num_positives, tf.int32)\n",
    "    \n",
    "    # Only keep the hardest negative samples\n",
    "    conf_loss_neg = conf_loss * tf.cast(neg_mask, tf.float32)\n",
    "    conf_loss_neg = tf.where(tf.math.is_nan(conf_loss_neg), tf.zeros_like(conf_loss_neg), conf_loss_neg) # prevent NaN values\n",
    "    conf_loss_neg, _ = tf.nn.top_k(tf.reshape(conf_loss_neg, (-1,)), k=tf.reduce_sum(num_negatives_keep))\n",
    "    conf_loss_neg = tf.reduce_mean(conf_loss_neg)\n",
    "    \n",
    "    # Compute the total loss (positive and negative)\n",
    "    conf_loss_pos = tf.reduce_sum(conf_loss * pos_mask, axis=-1)\n",
    "    total_loss = conf_loss_pos + conf_loss_neg\n",
    "    \n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Generate some test data\n",
    "batch_size = 2\n",
    "num_anchors = 3\n",
    "num_classes = 4\n",
    "num_coords = 4\n",
    "y_true = np.random.randint(0, 2, size=(batch_size, num_anchors, num_classes+1)).astype('float32')\n",
    "y_pred = np.random.rand(batch_size, num_anchors, num_classes+1).astype('float32')\n",
    "loc_true = np.random.rand(batch_size, num_anchors, num_coords).astype('float32')\n",
    "loc_pred = np.random.rand(batch_size, num_anchors, num_coords).astype('float32')\n",
    "weights = np.random.rand(batch_size, num_anchors).astype('float32')\n",
    "\n",
    "# Compute the expected losses\n",
    "expected_classification_loss = tf.keras.losses.binary_crossentropy(y_true[:,:,1:], y_pred[:,:,1:], from_logits=True)\n",
    "expected_classification_loss = tf.reduce_sum(expected_classification_loss * y_true[:,:,0], axis=-1)\n",
    "expected_classification_loss = tf.reduce_mean(expected_classification_loss)\n",
    "\n",
    "expected_localization_loss = tf.keras.losses.huber(loc_true, loc_pred, delta=1.0)\n",
    "expected_localization_loss = tf.reduce_sum(expected_localization_loss * weights, axis=-1)\n",
    "expected_localization_loss = tf.reduce_mean(expected_localization_loss)\n",
    "\n",
    "expected_hard_negative_mining_loss = expected_classification_loss\n",
    "\n",
    "expected_regularization_loss = tf.reduce_sum(tf.abs(y_pred[:,:,1:]), axis=-1)\n",
    "expected_regularization_loss = tf.reduce_mean(expected_regularization_loss)\n",
    "\n",
    "# Call each loss function on the test data\n",
    "classification_loss_value = confidence_loss(y_true, y_pred).numpy()\n",
    "localization_loss_value = localization_loss(loc_true, loc_pred, weights).numpy()\n",
    "hard_negative_mining_loss_value = hard_negative_mining_loss(y_true, y_pred).numpy()\n",
    "regularization_loss_value = regularization_loss(y_pred).numpy()\n",
    "\n",
    "# Compare the expected and actual loss values\n",
    "np.testing.assert_allclose(classification_loss_value, expected_classification_loss.numpy())\n",
    "np.testing.assert_allclose(localization_loss_value, expected_localization_loss.numpy())\n",
    "np.testing.assert_allclose(hard_negative_mining_loss_value, expected_hard_negative_mining_loss.numpy())\n",
    "np.testing.assert_allclose(regularization_loss_value, expected_regularization_loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10)\n",
    "\n",
    "\n",
    "# Plot the performance against all four losses\n",
    "plt.plot(history.history['conf_loss'], label='Confidence loss')\n",
    "plt.plot(history.history['loc_loss'], label='Localization loss')\n",
    "plt.plot(history.history['neg_loss'], label='Hard negative mining loss')\n",
    "plt.plot(history.history['reg_loss'], label='Regularization loss')\n",
    "plt.title('Training Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_predictions(predictions, confidence_threshold=0.5, nms_iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Post-processes the predictions to obtain the bounding boxes and class scores.\n",
    "    predictions: the output of the model.predict() function (batch_size, num_anchors, num_classes + 4)\n",
    "    confidence_threshold: the minimum confidence threshold for the predicted class scores\n",
    "    nms_iou_threshold: the intersection over union threshold for non-maximum suppression\n",
    "    \"\"\"\n",
    "    # Extract the predicted class scores and bounding box offsets\n",
    "    confidences = predictions[:,:,1:]\n",
    "    offsets = predictions[:,:,0:4]\n",
    "    \n",
    "    # Decode the anchor boxes\n",
    "    boxes = decode_boxes(offsets)\n",
    "    \n",
    "    # Apply the sigmoid activation function to the predicted class scores\n",
    "    confidences = tf.sigmoid(confidences)\n",
    "    \n",
    "    # Find the index of the class with the highest score for each anchor box\n",
    "    class_indices = tf.argmax(confidences, axis=-1)\n",
    "    \n",
    "    # Extract the highest class score for each anchor box\n",
    "    class_scores = tf.reduce_max(confidences, axis=-1)\n",
    "    \n",
    "    # Apply the confidence threshold\n",
    "    mask = class_scores >= confidence_threshold\n",
    "    boxes = tf.boolean_mask(boxes, mask)\n",
    "    class_indices = tf.boolean_mask(class_indices, mask)\n",
    "    class_scores = tf.boolean_mask(class_scores, mask)\n",
    "    \n",
    "    # Apply non-maximum suppression\n",
    "    selected_indices = tf.image.non_max_suppression(boxes, class_scores, max_output_size=100, iou_threshold=nms_iou_threshold)\n",
    "    selected_boxes = tf.gather(boxes, selected_indices)\n",
    "    selected_class_indices = tf.gather(class_indices, selected_indices)\n",
    "    selected_class_scores = tf.gather(class_scores, selected_indices)\n",
    "    \n",
    "    return selected_boxes, selected_class_scores, selected_class_indices\n",
    "\n",
    "\n",
    "def visualize_predictions(images, boxes, scores, class_indices, class_names):\n",
    "    \"\"\"\n",
    "    Visualizes the predicted bounding boxes on the test images.\n",
    "    images: the test images (batch_size, height, width, channels)\n",
    "    boxes: the predicted bounding boxes (num_boxes, 4)\n",
    "    scores: the predicted class scores (num_boxes,)\n",
    "    class_indices: the predicted class indices (num_boxes,)\n",
    "    class_names: a list of class names\n",
    "    \"\"\"\n",
    "    for i in range(images.shape[0]):\n",
    "        # Convert the image from BGR to RGB\n",
    "        image = cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert the boxes from center-size format to top-left corner format\n",
    "        boxes_i = convert_boxes(boxes[i])\n",
    "        \n",
    "        # Draw the predicted bounding boxes and class labels on the image\n",
    "        for box, score, class_idx in zip(boxes_i, scores[i], class_indices[i]):\n",
    "            x1, y1, x2, y2 = box\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            label = f\"{class_names[class_idx]}: {score:.2f}\"\n",
    "            cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "        \n",
    "        # Show the image\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('ssd300.h5')\n",
    "\n",
    "model = load_model('ssd300.h5', custom_objects={'confidence_loss': confidence_loss, 'localization_loss': localization_loss, 'hard_negative_mining_loss': hard_negative_mining_loss, 'regularization_loss': regularization_loss})\n",
    "\n",
    "# Load the test images\n",
    "images = []\n",
    "for i in range(1, 11):\n",
    "    image = cv2.imread(f\"images/test{i}.jpg\")\n",
    "    images.append(image)\n",
    "images = np.array(images)\n",
    "\n",
    "# Preprocess the images\n",
    "images = preprocess_images(images)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(images)\n",
    "\n",
    "# Post-process the predictions\n",
    "boxes, scores, class_indices = post_process_predictions(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
