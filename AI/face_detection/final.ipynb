{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (2.11.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (4.7.0.68)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7.0)\n",
      "Requirement already satisfied: imutils in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (0.5.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: keras in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (2.11.0)\n",
      "Requirement already satisfied: ssd in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (1.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.1.21)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.30.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.24.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: wmi in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from ssd) (1.5.1)\n",
      "Requirement already satisfied: pypiwin32>=154 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ssd) (223)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: pywin32>=223 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from pypiwin32>=154->ssd) (305)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.16.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --user tensorflow opencv-python matplotlib imutils scikit-learn keras ssd pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Harry Parker\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the dataset\n",
    "data_path = \"data/WIDERFace\"\n",
    "AUTOTUNE = 1000\n",
    "batch_size = 32\n",
    "\n",
    "# Define the paths to the train, test, and val sets\n",
    "train_path = os.path.join(data_path, \"WIDER_train\")\n",
    "test_path = os.path.join(data_path, \"WIDER_test\")\n",
    "val_path = os.path.join(data_path, \"WIDER_val\")\n",
    "labels_path = os.path.join(data_path, \"wider_face_split\")\n",
    "\n",
    "# Define the paths to the ground truth text files\n",
    "train_labels_path = os.path.join(train_path, \"wider_face_train_bbx_gt.txt\")\n",
    "test_labels_path = os.path.join(test_path, \"wider_face_test_filelist.txt\")\n",
    "val_labels_path = os.path.join(val_path, \"wider_face_val_bbx_gt.txt\")\n",
    "\n",
    "# Define a generator function to load images\n",
    "def load_images(image_paths):\n",
    "    for image_path in image_paths:\n",
    "        yield cv2.imread(image_path)\n",
    "\n",
    "# Define a generator function to load labels\n",
    "def load_labels(label_file_path):\n",
    "    with open(label_file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            # process the line to extract the label information\n",
    "            label_info = line.strip().split()\n",
    "            yield label_info\n",
    "\n",
    "# Load the train, test, and val images using the generator function\n",
    "train_images = load_images((os.path.join(root, file) for root, dirs, files in os.walk(train_path) for file in files if file.endswith(\".jpg\")))\n",
    "test_images = load_images((os.path.join(root, file) for root, dirs, files in os.walk(test_path) for file in files if file.endswith(\".jpg\")))\n",
    "val_images = load_images((os.path.join(root, file) for root, dirs, files in os.walk(val_path) for file in files if file.endswith(\".jpg\")))\n",
    "\n",
    "# Load the ground truth text files using the generator function\n",
    "train_labels = load_labels(train_labels_path)\n",
    "test_labels = load_labels(test_labels_path)\n",
    "val_labels = load_labels(val_labels_path)\n",
    "\n",
    "# Use TensorFlow's data pipeline to preprocess the data\n",
    "train_image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(train_path) for file in files if file.endswith(\".jpg\")]\n",
    "train_data = tf.data.Dataset.from_generator(load_images, args=[train_image_paths], output_types=tf.uint8, output_shapes=tf.TensorShape([None, None, 3]))\n",
    "train_data = train_data.map(lambda x: tf.image.convert_image_dtype(x, tf.float32))\n",
    "train_labels = tf.data.Dataset.from_generator(load_labels, args=[train_labels_path], output_types=tf.float32)\n",
    "train_dataset = tf.data.Dataset.zip((train_data, train_labels))\n",
    "train_dataset = train_dataset.batch(batch_size).repeat().prefetch(buffer_size=AUTOTUNE)\n",
    "steps_per_epoch = len(list((os.path.join(root, file) for root, dirs, files in os.walk(train_path) for file in files if file.endswith(\".jpg\")))) // batch_size\n",
    "\n",
    "# Load and preprocess test data images\n",
    "test_image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(test_path) for file in files if file.endswith(\".jpg\")]\n",
    "test_data = tf.data.Dataset.from_generator(load_images, args=[test_image_paths], output_types=tf.uint8, output_shapes=tf.TensorShape([None, None, 3]))\n",
    "test_data = test_data.map(lambda x: tf.image.convert_image_dtype(x, tf.float32))\n",
    "\n",
    "# Load test labels\n",
    "test_labels = tf.data.Dataset.from_generator(load_labels, args=[test_labels_path], output_types=tf.float32)\n",
    "\n",
    "# Concatenate test data and labels\n",
    "test_dataset = tf.data.Dataset.concatenate(test_data, test_labels)\n",
    "\n",
    "\n",
    "# Use TensorFlow's data pipeline to preprocess the data\n",
    "val_image_paths = [os.path.join(root, file) for root, dirs, files in os.walk(val_path) for file in files if file.endswith(\".jpg\")]\n",
    "val_data = tf.data.Dataset.from_generator(load_images, args=[val_image_paths], output_types=tf.uint8, output_shapes=tf.TensorShape([None, None, 3]))\n",
    "val_data = val_data.map(lambda x: tf.image.convert_image_dtype(x, tf.float32))\n",
    "val_labels = tf.data.Dataset.from_generator(load_labels, args=[val_labels_path], output_types=tf.float32)\n",
    "val_dataset = tf.data.Dataset.zip((val_data, val_labels))\n",
    "val_dataset = val_dataset.batch(batch_size).repeat().prefetch(buffer_size=AUTOTUNE)\n",
    "steps_per_epoch = len(list((os.path.join(root, file) for root, dirs, files in os.walk(train_path) for file in files if file.endswith(\".jpg\")))) // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "data_path = \"data/WIDERFace\"\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "\n",
    "# Define the paths to the train, test, and val sets\n",
    "train_path = os.path.join(data_path, \"WIDER_train\")\n",
    "test_path = os.path.join(data_path, \"WIDER_test\")\n",
    "val_path = os.path.join(data_path, \"WIDER_val\")\n",
    "\n",
    "\n",
    "train_labels_path = (\"data/WIDERFace/wider_face_split/wider_face_train_bbx_gt.txt\")\n",
    "test_labels_path = (\"data/WIDERFace/wider_face_split/wider_face_test_filelist.txt\")\n",
    "val_labels_path = (\"data/WIDERFace/wider_face_split/wider_face_val_bbx_gt.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_wider_annotations(annotations_file):\n",
    "    \"\"\"\n",
    "    Load annotations for each image in the WIDER Face dataset from a text file.\n",
    "    \n",
    "    Args:\n",
    "    - annotations_file: str, path to the annotations file.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary where the keys are image file names and the values are lists of bounding boxes for that image.\n",
    "    \"\"\"\n",
    "    annotations = {}\n",
    "    \n",
    "    with open(annotations_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Split the line into components\n",
    "        components = line.split(' ')\n",
    "        image_path = components[0]\n",
    "        bbox = list(map(int, components[1:]))\n",
    "        \n",
    "        # Convert the image path to a file name\n",
    "        image_file = os.path.basename(image_path)\n",
    "        \n",
    "        # Add the bbox to the list of bboxes for this image\n",
    "        if image_file in annotations:\n",
    "            annotations[image_path].append(bbox)\n",
    "        else:\n",
    "            annotations[image_path] = [bbox]\n",
    "    \n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 12880\n",
      "Number of test images: 16097\n",
      "Number of validation images: 3226\n",
      "data\\WIDERFace\\WIDER_train\\images/0--Parade/0_Parade_marchingband_1_483.jpg\n",
      "[[9, 485, 419, 48, 65, 1]]\n",
      "('data\\\\WIDERFace\\\\WIDER_train\\\\images/0--Parade/0_Parade_marchingband_1_483.jpg', [[9, 485, 419, 48, 65, 1]])\n"
     ]
    }
   ],
   "source": [
    "train_annotations = load_wider_annotations(train_labels_path)\n",
    "test_annotations = load_wider_annotations(test_labels_path)\n",
    "val_annotations = load_wider_annotations(val_labels_path)\n",
    "\n",
    "print(\"Number of training images: {}\".format(len(train_annotations)))\n",
    "print(\"Number of test images: {}\".format(len(test_annotations)))\n",
    "print(\"Number of validation images: {}\".format(len(val_annotations)))\n",
    "\n",
    "print(list(train_annotations.keys())[22])\n",
    "print(list(train_annotations.values())[22])\n",
    "print(list(train_annotations.items())[22])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "def read_image(image_path):\n",
    "    byte_img = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(byte_img)\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = tf.image.resize(image, (224, 224, 3))\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "def create_dataset(data_dir, annotations, split, batch_size, shuffle):\n",
    "    if split == 'train':\n",
    "        image_dir = os.path.join(data_dir, 'WIDER_train', 'images')\n",
    "    elif split == 'val':\n",
    "        image_dir = os.path.join(data_dir, 'WIDER_val', 'images')\n",
    "    elif split == 'test':\n",
    "        image_dir = os.path.join(data_dir, 'WIDER_test', 'images')\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid split: {split}. Must be 'train', 'val', or 'test'.\")\n",
    "\n",
    "    image_paths = [os.path.join(key) for key in list(annotations.keys())]\n",
    "\n",
    "    print(f\"Creating {split} dataset with {len(image_paths)} images.\")\n",
    "\n",
    "    def process_image(image_path):\n",
    "        image = read_image(image_path)\n",
    "        return image\n",
    "\n",
    "    def process_labels(key):\n",
    "        key = key.numpy().tolist()\n",
    "        label = annotations[key]\n",
    "        return label\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths))\n",
    "    dataset = dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x: (x, tf.zeros_like(x)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x, y: (x, tf.py_function(process_labels, [y], tf.int64)), num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image(filename):\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image_string)\n",
    "    image = tf.image.resize(image, [300, 300])\n",
    "    return image\n",
    "\n",
    "def parse_label(filename, annotations):\n",
    "    return annotations[os.path.basename(filename.numpy().decode('utf-8'))]\n",
    "\n",
    "def create_dataset2(annotations_path, shuffle=True):\n",
    "    annotations = load_wider_annotations(annotations_path)\n",
    "    image_paths = tf.constant([os.path.join(filename) for filename in annotations.keys()])\n",
    "    labels = tf.constant([annotations[filename] for filename in annotations.keys()])\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(annotations))\n",
    "    dataset = dataset.map(lambda filename, label: (parse_image(filename), label), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (300, 300, 3)\n",
      "Label: tf.Tensor([[  6 766 104  44  58   1]], shape=(1, 6), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12880, 3226, 16097)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = create_dataset2(train_labels_path, shuffle=True)\n",
    "val_dataset = create_dataset2(val_labels_path, shuffle=False)\n",
    "test_dataset = create_dataset2(test_labels_path, shuffle=False)\n",
    "\n",
    "# Retrieve the first element from the dataset\n",
    "example = next(iter(train_dataset))\n",
    "\n",
    "# Print the contents of the element\n",
    "print('Image shape:', example[0].shape)\n",
    "print('Label:', example[1])\n",
    "\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_with_label(image, label):\n",
    "    # Plot the image\n",
    "    plt.imshow(image.numpy().astype(\"uint8\"))\n",
    "    # Draw the bounding box on top of the image\n",
    "    class_id = label[0]\n",
    "    xmin, ymin, xmax, ymax = label[1:]\n",
    "    if class_id == 1:\n",
    "        rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                             fill=False, color=\"red\")\n",
    "        plt.gca().add_patch(rect)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jun 21 12:43:54 2018\n",
    "\n",
    "@author: shen1994\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "class Generator(object):\n",
    "    def __init__(self, bbox_util, batch_size, image_size, \n",
    "                 train_gt, val_gt, train_keys, val_keys, \n",
    "                 saturation_var=0.5,\n",
    "                 brightness_var=0.5,\n",
    "                 contrast_var=0.5,\n",
    "                 lighting_std=0.5,\n",
    "                 hflip_prob=0.0,\n",
    "                 do_crop=True,\n",
    "                 crop_area_range=[0.75, 1.0],\n",
    "                 aspect_ratio_range=[3./4., 4./3.]):\n",
    "        self.train_gt = train_gt\n",
    "        self.val_gt = val_gt\n",
    "        self.bbox_util = bbox_util\n",
    "        self.batch_size = batch_size\n",
    "        self.train_keys = train_keys\n",
    "        self.val_keys = val_keys\n",
    "        self.train_batches = len(train_keys)\n",
    "        self.val_batches = len(val_keys)\n",
    "        self.image_size = image_size\n",
    "        self.color_jitter = []\n",
    "        if saturation_var:\n",
    "            self.saturation_var = saturation_var\n",
    "            self.color_jitter.append(self.saturation)\n",
    "        if brightness_var:\n",
    "            self.brightness_var = brightness_var\n",
    "            self.color_jitter.append(self.brightness)\n",
    "        if contrast_var:\n",
    "            self.contrast_var = contrast_var\n",
    "            self.color_jitter.append(self.contrast)\n",
    "        self.lighting_std = lighting_std\n",
    "        self.hflip_prob = hflip_prob\n",
    "        self.do_crop = do_crop\n",
    "        self.crop_area_range = crop_area_range\n",
    "        self.aspect_ratio_range = aspect_ratio_range\n",
    "    \n",
    "    def grayscale(self, rgb):\n",
    "        return rgb.dot([0.299, 0.587, 0.114])\n",
    "\n",
    "    def saturation(self, rgb):\n",
    "        gs = self.grayscale(rgb)\n",
    "        alpha = 2 * np.random.random() * self.saturation_var \n",
    "        alpha += 1 - self.saturation_var\n",
    "        rgb = rgb * alpha + (1 - alpha) * gs[:, :, None]\n",
    "        return np.clip(rgb, 0, 255)\n",
    "\n",
    "    def brightness(self, rgb):\n",
    "        alpha = 2 * np.random.random() * self.brightness_var \n",
    "        alpha += 1 - self.saturation_var\n",
    "        rgb = rgb * alpha\n",
    "        return np.clip(rgb, 0, 255)\n",
    "\n",
    "    def contrast(self, rgb):\n",
    "        gs = self.grayscale(rgb).mean() * np.ones_like(rgb)\n",
    "        alpha = 2 * np.random.random() * self.contrast_var \n",
    "        alpha += 1 - self.contrast_var\n",
    "        rgb = rgb * alpha + (1 - alpha) * gs\n",
    "        return np.clip(rgb, 0, 255)\n",
    "\n",
    "    def lighting(self, img):\n",
    "        cov = np.cov(img.reshape(-1, 3) / 255.0, rowvar=False)\n",
    "        eigval, eigvec = np.linalg.eigh(cov)\n",
    "        noise = np.random.randn(3) * self.lighting_std\n",
    "        noise = eigvec.dot(eigval * noise) * 255\n",
    "        img += noise\n",
    "        return np.clip(img, 0, 255)\n",
    "  \n",
    "    def horizontal_flip(self, img, y):\n",
    "        rand = np.random.random()\n",
    "        if  rand < self.hflip_prob and y.shape[0] > 0:\n",
    "            img = img[:, ::-1]\n",
    "            y_temp = y[:, 0]\n",
    "            y[:, 0] = [1.0] - y[:, 2]\n",
    "            y[:, 2] = [1.0] - y_temp\n",
    "\n",
    "        return img, y\n",
    " \n",
    "    def random_sized_crop(self, img, targets):\n",
    "        img_w = img.shape[1]\n",
    "        img_h = img.shape[0]\n",
    "        img_area = img_w * img_h\n",
    "        random_scale = np.random.random()\n",
    "        random_scale *= (self.crop_area_range[1] -\n",
    "                         self.crop_area_range[0])\n",
    "        random_scale += self.crop_area_range[0]\n",
    "        target_area = random_scale * img_area\n",
    "        random_ratio = np.random.random()\n",
    "        random_ratio *= (self.aspect_ratio_range[1] -\n",
    "                         self.aspect_ratio_range[0])\n",
    "        random_ratio += self.aspect_ratio_range[0]\n",
    "        w = np.round(np.sqrt(target_area * random_ratio))     \n",
    "        h = np.round(np.sqrt(target_area / random_ratio))\n",
    "        if np.random.random() < 0.5:\n",
    "            w, h = h, w\n",
    "        w = min(w, img_w)\n",
    "        w_rel = w / img_w\n",
    "        w = int(w)\n",
    "        h = min(h, img_h)\n",
    "        h_rel = h / img_h\n",
    "        h = int(h)\n",
    "        x = np.random.random() * (img_w - w)\n",
    "        x_rel = x / img_w\n",
    "        x = int(x)\n",
    "        y = np.random.random() * (img_h - h)\n",
    "        y_rel = y / img_h\n",
    "        y = int(y)\n",
    "        img = img[y:y+h, x:x+w]\n",
    "        new_targets = []\n",
    "        for box in targets:\n",
    "            cx = 0.5 * (box[0] + box[2])\n",
    "            cy = 0.5 * (box[1] + box[3])\n",
    "            if (x_rel < cx < x_rel + w_rel and\n",
    "                y_rel < cy < y_rel + h_rel):\n",
    "                xmin = (box[0] - x_rel) / w_rel\n",
    "                ymin = (box[1] - y_rel) / h_rel\n",
    "                xmax = (box[2] - x_rel) / w_rel\n",
    "                ymax = (box[3] - y_rel) / h_rel\n",
    "                xmin = max(0, xmin)\n",
    "                ymin = max(0, ymin)\n",
    "                xmax = min(1, xmax)\n",
    "                ymax = min(1, ymax)\n",
    "                box[:4] = [xmin, ymin, xmax, ymax]\n",
    "                new_targets.append(box)\n",
    "        new_targets = np.asarray(new_targets).reshape(-1, targets.shape[1])\n",
    "        return img, new_targets\n",
    "   \n",
    "    def generate(self, train=True):        \n",
    "        while True:\n",
    "            if train:\n",
    "                random.shuffle(self.train_keys)\n",
    "                keys = self.train_keys\n",
    "            else:\n",
    "                random.shuffle(self.val_keys)\n",
    "                keys = self.val_keys\n",
    "            inputs = []\n",
    "            targets = []\n",
    "            for key in keys:            \n",
    "                img_path = key\n",
    "                img = cv2.imread(img_path).astype('float32')\n",
    "                if train:\n",
    "                    y = self.train_gt[key].copy()\n",
    "                else:\n",
    "                    y = self.val_gt[key].copy()\n",
    "                if train and self.do_crop:\n",
    "                    img, y = self.random_sized_crop(img, y)\n",
    "                img = cv2.resize(img, self.image_size).astype('float32')\n",
    "                if train:\n",
    "                    random.shuffle(self.color_jitter)\n",
    "                    for jitter in self.color_jitter:\n",
    "                        img = jitter(img)\n",
    "                    if self.lighting_std:\n",
    "                        img = self.lighting(img)\n",
    "                    if self.hflip_prob > 0:\n",
    "                        img, y = self.horizontal_flip(img, y)\n",
    "                y = self.bbox_util.assign_boxes(y)\n",
    "                inputs.append(img)                \n",
    "                targets.append(y)\n",
    "                if len(targets) == self.batch_size:\n",
    "                    tmp_inp = np.array(inputs)\n",
    "                    tmp_targets = np.array(targets)\n",
    "                    inputs = []\n",
    "                    targets = []\n",
    "                    yield preprocess_input(tmp_inp), tmp_targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Some utils for SSD.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class BBoxUtility(object):\n",
    "    \"\"\"Utility class to do some stuff with bounding boxes and priors.\n",
    "\n",
    "    # Arguments\n",
    "        num_classes: Number of classes including background.\n",
    "        priors: Priors and variances, numpy tensor of shape (num_priors, 8),\n",
    "            priors[i] = [xmin, ymin, xmax, ymax, varxc, varyc, varw, varh].\n",
    "        overlap_threshold: Threshold to assign box to a prior.\n",
    "        nms_thresh: Nms threshold.\n",
    "        top_k: Number of total bboxes to be kept per image after nms step.\n",
    "\n",
    "    # References\n",
    "        https://arxiv.org/abs/1512.02325\n",
    "    \"\"\"\n",
    "    # TODO add setter methods for nms_thresh and top_K\n",
    "    def __init__(self, num_classes, priors=None, overlap_threshold=0.5,\n",
    "                 nms_thresh=0.45, top_k=400, is_decoder=False):\n",
    "        self.num_classes = num_classes\n",
    "        self.priors = priors\n",
    "        self.num_priors = 0 if priors is None else len(priors)\n",
    "        self.overlap_threshold = overlap_threshold\n",
    "        self._nms_thresh = nms_thresh\n",
    "        self._top_k = top_k\n",
    "        self.boxes = tf.placeholder(dtype='float32', shape=(None, 4))\n",
    "        self.scores = tf.placeholder(dtype='float32', shape=(None,))\n",
    "        self.nms = tf.image.non_max_suppression(self.boxes, self.scores,\n",
    "                                                self._top_k,\n",
    "                                                iou_threshold=self._nms_thresh)\n",
    "        if is_decoder:\n",
    "            self.sess = K.get_session()\n",
    "\n",
    "    @property\n",
    "    def nms_thresh(self):\n",
    "        return self._nms_thresh\n",
    "\n",
    "    @nms_thresh.setter\n",
    "    def nms_thresh(self, value):\n",
    "        self._nms_thresh = value\n",
    "        self.nms = tf.image.non_max_suppression(self.boxes, self.scores,\n",
    "                                                self._top_k,\n",
    "                                                iou_threshold=self._nms_thresh)\n",
    "\n",
    "    @property\n",
    "    def top_k(self):\n",
    "        return self._top_k\n",
    "\n",
    "    @top_k.setter\n",
    "    def top_k(self, value):\n",
    "        self._top_k = value\n",
    "        self.nms = tf.image.non_max_suppression(self.boxes, self.scores,\n",
    "                                                self._top_k,\n",
    "                                                iou_threshold=self._nms_thresh)\n",
    "\n",
    "    def iou(self, box):\n",
    "        \"\"\"Compute intersection over union for the box with all priors.\n",
    "\n",
    "        # Arguments\n",
    "            box: Box, numpy tensor of shape (4,).\n",
    "\n",
    "        # Return\n",
    "            iou: Intersection over union,\n",
    "                numpy tensor of shape (num_priors).\n",
    "        \"\"\"\n",
    "        # compute intersection\n",
    "        inter_upleft = np.maximum(self.priors[:, :2], box[:2])\n",
    "        inter_botright = np.minimum(self.priors[:, 2:4], box[2:])\n",
    "        inter_wh = inter_botright - inter_upleft\n",
    "        inter_wh = np.maximum(inter_wh, 0)\n",
    "        inter = inter_wh[:, 0] * inter_wh[:, 1]\n",
    "        # compute union\n",
    "        area_pred = (box[2] - box[0]) * (box[3] - box[1])\n",
    "        area_gt = (self.priors[:, 2] - self.priors[:, 0])\n",
    "        area_gt *= (self.priors[:, 3] - self.priors[:, 1])\n",
    "        union = area_pred + area_gt - inter\n",
    "        # compute iou\n",
    "        iou = inter / union\n",
    "        return iou\n",
    "\n",
    "    def encode_box(self, box, return_iou=True):\n",
    "        \"\"\"Encode box for training, do it only for assigned priors.\n",
    "\n",
    "        # Arguments\n",
    "            box: Box, numpy tensor of shape (4,).\n",
    "            return_iou: Whether to concat iou to encoded values.\n",
    "\n",
    "        # Return\n",
    "            encoded_box: Tensor with encoded box\n",
    "                numpy tensor of shape (num_priors, 4 + int(return_iou)).\n",
    "        \"\"\"\n",
    "        iou = self.iou(box)\n",
    "        encoded_box = np.zeros((self.num_priors, 4 + return_iou))\n",
    "        assign_mask = iou > self.overlap_threshold\n",
    "        if not assign_mask.any():\n",
    "            assign_mask[iou.argmax()] = True\n",
    "        if return_iou:\n",
    "            encoded_box[:, -1][assign_mask] = iou[assign_mask]\n",
    "        assigned_priors = self.priors[assign_mask]\n",
    "        box_center = 0.5 * (box[:2] + box[2:])\n",
    "        box_wh = box[2:] - box[:2]\n",
    "        assigned_priors_center = 0.5 * (assigned_priors[:, :2] +\n",
    "                                        assigned_priors[:, 2:4])\n",
    "        assigned_priors_wh = (assigned_priors[:, 2:4] -\n",
    "                              assigned_priors[:, :2])\n",
    "        # we encode variance\n",
    "        encoded_box[:, :2][assign_mask] = box_center - assigned_priors_center\n",
    "        encoded_box[:, :2][assign_mask] /= assigned_priors_wh\n",
    "        encoded_box[:, :2][assign_mask] /= assigned_priors[:, -4:-2]\n",
    "        encoded_box[:, 2:4][assign_mask] = np.log(box_wh /\n",
    "                                                  assigned_priors_wh)\n",
    "        encoded_box[:, 2:4][assign_mask] /= assigned_priors[:, -2:]\n",
    "\n",
    "        return encoded_box\n",
    "\n",
    "    def assign_boxes(self, boxes):\n",
    "        \"\"\"Assign boxes to priors for training.\n",
    "\n",
    "        # Arguments\n",
    "            boxes: Box, numpy tensor of shape (num_boxes, 4 + num_classes),\n",
    "                num_classes without background.\n",
    "\n",
    "        # Return\n",
    "            assignment: Tensor with assigned boxes,\n",
    "                numpy tensor of shape (num_boxes, 4 + num_classes + 8),\n",
    "                priors in ground truth are fictitious,\n",
    "                assignment[:, -8] has 1 if prior should be penalized\n",
    "                    or in other words is assigned to some ground truth box,\n",
    "                assignment[:, -7:] are all 0. See loss for more details.\n",
    "        \"\"\"\n",
    "        assignment = np.zeros((self.num_priors, 4 + self.num_classes + 8))\n",
    "        assignment[:, 4] = 1.0\n",
    "        if len(boxes) == 0:\n",
    "            return assignment\n",
    "        encoded_boxes = np.apply_along_axis(self.encode_box, 1, boxes[:, :4])\n",
    "        encoded_boxes = encoded_boxes.reshape(-1, self.num_priors, 5)\n",
    "        best_iou = encoded_boxes[:, :, -1].max(axis=0)\n",
    "        best_iou_idx = encoded_boxes[:, :, -1].argmax(axis=0)\n",
    "        best_iou_mask = best_iou > 0\n",
    "        best_iou_idx = best_iou_idx[best_iou_mask]\n",
    "        assign_num = len(best_iou_idx)\n",
    "        encoded_boxes = encoded_boxes[:, best_iou_mask, :]\n",
    "        assignment[:, :4][best_iou_mask] = encoded_boxes[best_iou_idx,\n",
    "                                                         np.arange(assign_num),\n",
    "                                                         :4]       \n",
    "        assignment[:, 4][best_iou_mask] = 0\n",
    "        assignment[:, 5:-8][best_iou_mask] = boxes[best_iou_idx, 4:]\n",
    "        assignment[:, -8][best_iou_mask] = 1\n",
    "        return assignment\n",
    "\n",
    "    def decode_boxes(self, mbox_loc, mbox_priorbox, variances):\n",
    "        \"\"\"Convert bboxes from local predictions to shifted priors.\n",
    "\n",
    "        # Arguments\n",
    "            mbox_loc: Numpy array of predicted locations.\n",
    "            mbox_priorbox: Numpy array of prior boxes.\n",
    "            variances: Numpy array of variances.\n",
    "\n",
    "        # Return\n",
    "            decode_bbox: Shifted priors.\n",
    "        \"\"\"\n",
    "        prior_width = mbox_priorbox[:, 2] - mbox_priorbox[:, 0]\n",
    "        prior_height = mbox_priorbox[:, 3] - mbox_priorbox[:, 1]\n",
    "        prior_center_x = 0.5 * (mbox_priorbox[:, 2] + mbox_priorbox[:, 0])\n",
    "        prior_center_y = 0.5 * (mbox_priorbox[:, 3] + mbox_priorbox[:, 1])\n",
    "        decode_bbox_center_x = mbox_loc[:, 0] * prior_width * variances[:, 0]\n",
    "        decode_bbox_center_x += prior_center_x\n",
    "        decode_bbox_center_y = mbox_loc[:, 1] * prior_width * variances[:, 1]\n",
    "        decode_bbox_center_y += prior_center_y\n",
    "        decode_bbox_width = np.exp(mbox_loc[:, 2] * variances[:, 2])\n",
    "        decode_bbox_width *= prior_width\n",
    "        decode_bbox_height = np.exp(mbox_loc[:, 3] * variances[:, 3])\n",
    "        decode_bbox_height *= prior_height\n",
    "        decode_bbox_xmin = decode_bbox_center_x - 0.5 * decode_bbox_width\n",
    "        decode_bbox_ymin = decode_bbox_center_y - 0.5 * decode_bbox_height\n",
    "        decode_bbox_xmax = decode_bbox_center_x + 0.5 * decode_bbox_width\n",
    "        decode_bbox_ymax = decode_bbox_center_y + 0.5 * decode_bbox_height\n",
    "        decode_bbox = np.concatenate((decode_bbox_xmin[:, None],\n",
    "                                      decode_bbox_ymin[:, None],\n",
    "                                      decode_bbox_xmax[:, None],\n",
    "                                      decode_bbox_ymax[:, None]), axis=-1)\n",
    "        decode_bbox = np.minimum(np.maximum(decode_bbox, 0.0), 1.0)\n",
    "        return decode_bbox\n",
    "\n",
    "    def detection_out(self, predictions, background_label_id=0, keep_top_k=200,\n",
    "                      confidence_threshold=0.01):\n",
    "        \"\"\"Do non maximum suppression (nms) on prediction results.\n",
    "\n",
    "        # Arguments\n",
    "            predictions: Numpy array of predicted values.\n",
    "            num_classes: Number of classes for prediction.\n",
    "            background_label_id: Label of background class.\n",
    "            keep_top_k: Number of total bboxes to be kept per image\n",
    "                after nms step.\n",
    "            confidence_threshold: Only consider detections,\n",
    "                whose confidences are larger than a threshold.\n",
    "\n",
    "        # Return\n",
    "            results: List of predictions for every picture. Each prediction is:\n",
    "                [label, confidence, xmin, ymin, xmax, ymax]\n",
    "        \"\"\"\n",
    "        mbox_loc = predictions[:, :, :4]\n",
    "        variances = predictions[:, :, -4:]\n",
    "        mbox_priorbox = predictions[:, :, -8:-4]\n",
    "        mbox_conf = predictions[:, :, 4:-8]\n",
    "        results = []\n",
    "        for i in range(len(mbox_loc)):\n",
    "            results.append([])\n",
    "            decode_bbox = self.decode_boxes(mbox_loc[i],\n",
    "                                            mbox_priorbox[i], variances[i])\n",
    "            for c in range(self.num_classes):\n",
    "                if c == background_label_id:\n",
    "                    continue\n",
    "                c_confs = mbox_conf[i, :, c]\n",
    "                c_confs_m = c_confs > confidence_threshold\n",
    "                if len(c_confs[c_confs_m]) > 0:\n",
    "                    boxes_to_process = decode_bbox[c_confs_m]\n",
    "                    confs_to_process = c_confs[c_confs_m]\n",
    "                    feed_dict = {self.boxes: boxes_to_process,\n",
    "                                 self.scores: confs_to_process}\n",
    "                    idx = self.sess.run(self.nms, feed_dict=feed_dict)\n",
    "                    good_boxes = boxes_to_process[idx]\n",
    "                    confs = confs_to_process[idx][:, None]\n",
    "                    labels = c * np.ones((len(idx), 1))\n",
    "                    c_pred = np.concatenate((labels, confs, good_boxes),\n",
    "                                            axis=1)\n",
    "                    results[-1].extend(c_pred)\n",
    "            if len(results[-1]) > 0:\n",
    "                results[-1] = np.array(results[-1])\n",
    "                argsort = np.argsort(results[-1][:, 1])[::-1]\n",
    "                results[-1] = results[-1][argsort]\n",
    "                results[-1] = results[-1][:keep_top_k]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Some utils for SSD.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class BBoxUtility(object):\n",
    "    \"\"\"Utility class to do some stuff with bounding boxes and priors.\n",
    "\n",
    "    # Arguments\n",
    "        num_classes: Number of classes including background.\n",
    "        priors: Priors and variances, numpy tensor of shape (num_priors, 8),\n",
    "            priors[i] = [xmin, ymin, xmax, ymax, varxc, varyc, varw, varh].\n",
    "        overlap_threshold: Threshold to assign box to a prior.\n",
    "        nms_thresh: Nms threshold.\n",
    "        top_k: Number of total bboxes to be kept per image after nms step.\n",
    "\n",
    "    # References\n",
    "        https://arxiv.org/abs/1512.02325\n",
    "    \"\"\"\n",
    "    # TODO add setter methods for nms_thresh and top_K\n",
    "    def __init__(self, session, num_classes, nms_thresh=0.32, top_k=15):\n",
    "        self.num_classes = num_classes\n",
    "        self._nms_thresh = nms_thresh\n",
    "        self._top_k = top_k\n",
    "        self.boxes = tf.placeholder(dtype='float32', shape=(None, 4))\n",
    "        self.scores = tf.placeholder(dtype='float32', shape=(None,))\n",
    "        self.nms = tf.image.non_max_suppression(self.boxes, self.scores,\n",
    "                                                self._top_k,\n",
    "                                                iou_threshold=self._nms_thresh)\n",
    "        self.sess = session\n",
    "\n",
    "    def decode_boxes(self, mbox_loc, mbox_priorbox, variances):\n",
    "        \"\"\"Convert bboxes from local predictions to shifted priors.\n",
    "\n",
    "        # Arguments\n",
    "            mbox_loc: Numpy array of predicted locations.\n",
    "            mbox_priorbox: Numpy array of prior boxes.\n",
    "            variances: Numpy array of variances.\n",
    "\n",
    "        # Return\n",
    "            decode_bbox: Shifted priors.\n",
    "        \"\"\"\n",
    "        prior_width = mbox_priorbox[:, 2] - mbox_priorbox[:, 0]\n",
    "        prior_height = mbox_priorbox[:, 3] - mbox_priorbox[:, 1]\n",
    "        prior_center_x = 0.5 * (mbox_priorbox[:, 2] + mbox_priorbox[:, 0])\n",
    "        prior_center_y = 0.5 * (mbox_priorbox[:, 3] + mbox_priorbox[:, 1])\n",
    "        decode_bbox_center_x = mbox_loc[:, 0] * prior_width * variances[:, 0]\n",
    "        decode_bbox_center_x += prior_center_x\n",
    "        decode_bbox_center_y = mbox_loc[:, 1] * prior_width * variances[:, 1]\n",
    "        decode_bbox_center_y += prior_center_y\n",
    "        decode_bbox_width = np.exp(mbox_loc[:, 2] * variances[:, 2])\n",
    "        decode_bbox_width *= prior_width\n",
    "        decode_bbox_height = np.exp(mbox_loc[:, 3] * variances[:, 3])\n",
    "        decode_bbox_height *= prior_height\n",
    "        decode_bbox_xmin = decode_bbox_center_x - 0.5 * decode_bbox_width\n",
    "        decode_bbox_ymin = decode_bbox_center_y - 0.5 * decode_bbox_height\n",
    "        decode_bbox_xmax = decode_bbox_center_x + 0.5 * decode_bbox_width\n",
    "        decode_bbox_ymax = decode_bbox_center_y + 0.5 * decode_bbox_height\n",
    "        decode_bbox = np.concatenate((decode_bbox_xmin[:, None],\n",
    "                                      decode_bbox_ymin[:, None],\n",
    "                                      decode_bbox_xmax[:, None],\n",
    "                                      decode_bbox_ymax[:, None]), axis=-1)\n",
    "        decode_bbox = np.minimum(np.maximum(decode_bbox, 0.0), 1.0)\n",
    "        return decode_bbox\n",
    "\n",
    "    def detection_out(self, predictions, confidence_threshold=0.01):\n",
    "        \"\"\"Do non maximum suppression (nms) on prediction results.\n",
    "\n",
    "        # Arguments\n",
    "            predictions: Numpy array of predicted values.\n",
    "            num_classes: Number of classes for prediction.\n",
    "            background_label_id: Label of background class.\n",
    "            keep_top_k: Number of total bboxes to be kept per image\n",
    "                after nms step.\n",
    "            confidence_threshold: Only consider detections,\n",
    "                whose confidences are larger than a threshold.\n",
    "\n",
    "        # Return\n",
    "            results: List of predictions for every picture. Each prediction is:\n",
    "                [label, confidence, xmin, ymin, xmax, ymax]\n",
    "        \"\"\"\n",
    "        mbox_loc = predictions[:, :, :4]\n",
    "        variances = predictions[:, :, -4:]\n",
    "        mbox_priorbox = predictions[:, :, -8:-4]\n",
    "        mbox_conf = predictions[:, :, 4:-8]\n",
    "        results = []\n",
    "        for i in range(len(mbox_loc)):\n",
    "            results.append([])\n",
    "            decode_bbox = self.decode_boxes(mbox_loc[i],\n",
    "                                            mbox_priorbox[i], variances[i])\n",
    "            c_confs = mbox_conf[i, :, 1]\n",
    "            c_confs_m = c_confs > 0.01\n",
    "            if len(c_confs[c_confs_m]) > 0:\n",
    "                boxes_to_process = decode_bbox[c_confs_m]\n",
    "                confs_to_process = c_confs[c_confs_m]\n",
    "                feed_dict = {self.boxes: boxes_to_process,\n",
    "                             self.scores: confs_to_process}\n",
    "                idx = self.sess.run(self.nms, feed_dict=feed_dict)\n",
    "                good_boxes = boxes_to_process[idx]\n",
    "                confs = confs_to_process[idx][:, None]\n",
    "                labels = 1 * np.ones((len(idx), 1))\n",
    "                c_pred = np.concatenate((labels, confs, good_boxes),\n",
    "                         axis=1)\n",
    "                results[-1].extend(c_pred)\n",
    "\n",
    "            if len(results[-1]) > 0:\n",
    "                results[-1] = np.array(results[-1])\n",
    "                argsort = np.argsort(results[-1][:, 1])[::-1]\n",
    "                results[-1] = results[-1][argsort]\n",
    "                results[-1] = results[-1][:self._top_k]\n",
    "        return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, MaxPooling2D, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.applications import VGG16\n",
    "\n",
    "# Step 1: Choose a base network architecture\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(300, 300, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Some special pupropse layers for SSD.\"\"\"\n",
    "\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import InputSpec, Layer\n",
    "\n",
    "class Normalize(Layer):\n",
    "    \"\"\"Normalization layer as described in ParseNet paper.\n",
    "\n",
    "    # Arguments\n",
    "        scale: Default feature scale.\n",
    "\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n",
    "\n",
    "    # Output shape\n",
    "        Same as input\n",
    "\n",
    "    # References\n",
    "        http://cs.unc.edu/~wliu/papers/parsenet.pdf\n",
    "\n",
    "    #TODO\n",
    "        Add possibility to have one scale for all features.\n",
    "    \"\"\"\n",
    "    def __init__(self, scale, **kwargs):\n",
    "        if K.image_data_format() == 'tf':\n",
    "            self.axis = 3\n",
    "        else:\n",
    "            self.axis = 1\n",
    "        self.scale = scale\n",
    "        super(Normalize, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (input_shape[self.axis],)\n",
    "        init_gamma = self.scale * np.ones(shape)\n",
    "        self.gamma = K.variable(init_gamma, name='{}_gamma'.format(self.name))\n",
    "        self.trainable_saved_weights = [self.gamma]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        output = K.l2_normalize(x, self.axis)\n",
    "        output *= self.gamma\n",
    "        return output\n",
    "\n",
    "\n",
    "class PriorBox(Layer):\n",
    "    \"\"\"Generate the prior boxes of designated sizes and aspect ratios.\n",
    "\n",
    "    # Arguments\n",
    "        img_size: Size of the input image as tuple (w, h).\n",
    "        min_size: Minimum box size in pixels.\n",
    "        max_size: Maximum box size in pixels.\n",
    "        aspect_ratios: List of aspect ratios of boxes.\n",
    "        flip: Whether to consider reverse aspect ratios.\n",
    "        variances: List of variances for x, y, w, h.\n",
    "        clip: Whether to clip the prior's coordinates\n",
    "            such that they are within [0, 1].\n",
    "\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n",
    "\n",
    "    # Output shape\n",
    "        3D tensor with shape:\n",
    "        (samples, num_boxes, 8)\n",
    "\n",
    "    # References\n",
    "        https://arxiv.org/abs/1512.02325\n",
    "\n",
    "    #TODO\n",
    "        Add possibility not to have variances.\n",
    "        Add Theano support\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, min_size, max_size=None, aspect_ratios=None,\n",
    "                 flip=True, variances=[0.1], clip=True, **kwargs):\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            self.waxis = 2\n",
    "            self.haxis = 1\n",
    "        else:\n",
    "            self.waxis = 3\n",
    "            self.haxis = 2\n",
    "        self.img_size = img_size\n",
    "        if min_size <= 0:\n",
    "            raise Exception('min_size must be positive.')\n",
    "        self.min_size = min_size\n",
    "        self.max_size = max_size\n",
    "        self.aspect_ratios = [1.0]\n",
    "        if max_size:\n",
    "            if max_size < min_size:\n",
    "                raise Exception('max_size must be greater than min_size.')\n",
    "            self.aspect_ratios.append(1.0)\n",
    "        if aspect_ratios:\n",
    "            for ar in aspect_ratios:\n",
    "                if ar in self.aspect_ratios:\n",
    "                    continue\n",
    "                self.aspect_ratios.append(ar)\n",
    "                if flip:\n",
    "                    self.aspect_ratios.append(1.0 / ar)\n",
    "        self.variances = np.array(variances)\n",
    "        self.clip = True\n",
    "        super(PriorBox, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        num_priors_ = len(self.aspect_ratios)\n",
    "        layer_width = input_shape[self.waxis]\n",
    "        layer_height = input_shape[self.haxis]\n",
    "        num_boxes = num_priors_ * layer_width * layer_height\n",
    "        return (input_shape[0], num_boxes, 8)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if hasattr(x, '_keras_shape'):\n",
    "            input_shape = x._keras_shape\n",
    "        elif hasattr(K, 'int_shape'):\n",
    "            input_shape = K.int_shape(x)\n",
    "        layer_width = input_shape[self.waxis]\n",
    "        layer_height = input_shape[self.haxis]\n",
    "        img_width = self.img_size[0]\n",
    "        img_height = self.img_size[1]\n",
    "        # define prior boxes shapes\n",
    "        box_widths = []\n",
    "        box_heights = []\n",
    "        for ar in self.aspect_ratios:\n",
    "            if ar == 1 and len(box_widths) == 0:\n",
    "                box_widths.append(self.min_size)\n",
    "                box_heights.append(self.min_size)\n",
    "            elif ar == 1 and len(box_widths) > 0:\n",
    "                box_widths.append(np.sqrt(self.min_size * self.max_size))\n",
    "                box_heights.append(np.sqrt(self.min_size * self.max_size))\n",
    "            elif ar != 1:\n",
    "                box_widths.append(self.min_size * np.sqrt(ar))\n",
    "                box_heights.append(self.min_size / np.sqrt(ar))\n",
    "        box_widths = 0.5 * np.array(box_widths)\n",
    "        box_heights = 0.5 * np.array(box_heights)\n",
    "        # define centers of prior boxes\n",
    "        step_x = img_width / layer_width\n",
    "        step_y = img_height / layer_height\n",
    "        linx = np.linspace(0.5 * step_x, img_width - 0.5 * step_x,\n",
    "                           layer_width)\n",
    "        liny = np.linspace(0.5 * step_y, img_height - 0.5 * step_y,\n",
    "                           layer_height)\n",
    "        centers_x, centers_y = np.meshgrid(linx, liny)\n",
    "        centers_x = centers_x.reshape(-1, 1)\n",
    "        centers_y = centers_y.reshape(-1, 1)\n",
    "        # define xmin, ymin, xmax, ymax of prior boxes\n",
    "        num_priors_ = len(self.aspect_ratios)\n",
    "        prior_boxes = np.concatenate((centers_x, centers_y), axis=1)\n",
    "        prior_boxes = np.tile(prior_boxes, (1, 2 * num_priors_))\n",
    "        prior_boxes[:, 0::4] -= box_widths\n",
    "        prior_boxes[:, 1::4] -= box_heights\n",
    "        prior_boxes[:, 2::4] += box_widths\n",
    "        prior_boxes[:, 3::4] += box_heights\n",
    "        prior_boxes[:, 0::2] /= img_width\n",
    "        prior_boxes[:, 1::2] /= img_height\n",
    "        prior_boxes = prior_boxes.reshape(-1, 4)\n",
    "        if self.clip:\n",
    "            prior_boxes = np.minimum(np.maximum(prior_boxes, 0.0), 1.0)\n",
    "        # define variances\n",
    "        num_boxes = len(prior_boxes)\n",
    "        if len(self.variances) == 1:\n",
    "            variances = np.ones((num_boxes, 4)) * self.variances[0]\n",
    "        elif len(self.variances) == 4:\n",
    "            variances = np.tile(self.variances, (num_boxes, 1))\n",
    "        else:\n",
    "            raise Exception('Must provide one or four variances.')\n",
    "        prior_boxes = np.concatenate((prior_boxes, variances), axis=1)\n",
    "        \n",
    "        prior_boxes_shape = prior_boxes.shape\n",
    "        with open(\"images/priorbox\" \n",
    "                  + \"_\" + str(prior_boxes_shape[0])\n",
    "                  + \"_\" + str(prior_boxes_shape[1])\n",
    "                  + \".pkl\", \"wb\") as f:\n",
    "            pickle.dump(prior_boxes, f, pickle.HIGHEST_PROTOCOL)\n",
    "  \n",
    "        prior_boxes_tensor = K.expand_dims(K.variable(prior_boxes), 0)\n",
    "        if K.backend() == 'tensorflow':\n",
    "            pattern = [tf.shape(x)[0], 1, 1]\n",
    "            prior_boxes_tensor = tf.tile(prior_boxes_tensor, pattern)\n",
    "        elif K.backend() == 'theano':\n",
    "            #TODO\n",
    "            pass\n",
    "        return prior_boxes_tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Keras implementation of SSD.\"\"\"\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def SSD300(input_shape, num_classes=21):\n",
    "    \"\"\"SSD300 architecture.\n",
    "\n",
    "    # Arguments\n",
    "        input_shape: Shape of the input image,\n",
    "            expected to be either (300, 300, 3) or (3, 300, 300)(not tested).\n",
    "        num_classes: Number of classes including background.\n",
    "\n",
    "    # References\n",
    "        https://arxiv.org/abs/1512.02325\n",
    "    \"\"\"\n",
    "    m_scale = [0.10, 0.20, 0.38, 0.56, 0.74, 0.92, 1.1]\n",
    "\n",
    "    net = {}\n",
    "    # Block 1\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    img_size = (input_shape[1], input_shape[0])\n",
    "    net['input'] = input_tensor\n",
    "    net['conv1_1'] = Conv2D(64, (3, 3),\n",
    "                                   activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv1_1')(net['input'])\n",
    "    net['conv1_2'] = Conv2D(64, (3, 3),\n",
    "                                   activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv1_2')(net['conv1_1'])\n",
    "    net['pool1'] = MaxPooling2D((2, 2), strides=(2, 2), padding='same',\n",
    "                                name='pool1')(net['conv1_2'])\n",
    "    # Block 2\n",
    "    net['conv2_1'] = Conv2D(128, (3, 3),\n",
    "                                   activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv2_1')(net['pool1'])\n",
    "    net['conv2_2'] = Conv2D(128, (3, 3),\n",
    "                                   activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv2_2')(net['conv2_1'])\n",
    "    net['pool2'] = MaxPooling2D((2, 2), strides=(2, 2), padding='same',\n",
    "                                name='pool2')(net['conv2_2'])\n",
    "    # Block 3\n",
    "    net['conv3_1'] = Conv2D(192, (3, 3),\n",
    "                                   activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv3_1')(net['pool2'])\n",
    "    net['conv3_2'] = Conv2D(192, (3, 3),\n",
    "                                   activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv3_2')(net['conv3_1'])\n",
    "    net['conv3_3'] = Conv2D(192, (3, 3),\n",
    "                                   activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv3_3')(net['conv3_2'])\n",
    "    net['pool3'] = MaxPooling2D((2, 2), strides=(2, 2), padding='same',\n",
    "                                name='pool3')(net['conv3_3'])\n",
    "    # Block 4\n",
    "    net['conv4_1'] = Conv2D(256, (3, 3),\n",
    "                                   activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv4_1')(net['pool3'])\n",
    "    net['conv4_2'] = Conv2D(256, (3, 3),\n",
    "                                   activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv4_2')(net['conv4_1'])\n",
    "    net['conv4_3'] = Conv2D(256, (3, 3),\n",
    "                                   activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv4_3')(net['conv4_2'])\n",
    "    net['pool4'] = MaxPooling2D((2, 2), strides=(2, 2), padding='same',\n",
    "                                name='pool4')(net['conv4_3'])\n",
    "    # Block 5\n",
    "    net['conv5_1'] = Conv2D(256, (3, 3),\n",
    "                                   activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv5_1')(net['pool4'])\n",
    "    net['conv5_2'] = Conv2D(256, (3, 3),\n",
    "                                   activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv5_2')(net['conv5_1'])\n",
    "    net['conv5_3'] = Conv2D(256, (3, 3),\n",
    "                                   activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv5_3')(net['conv5_2'])\n",
    "    net['pool5'] = MaxPooling2D((3, 3), strides=(1, 1), padding='same',\n",
    "                                name='pool5')(net['conv5_3'])\n",
    "\n",
    "    net['fc6'] = Conv2D(512, (3, 3), dilation_rate=(6, 6),\n",
    "                                     activation='relu', padding='same',\n",
    "                                     name='fc6')(net['pool5'])\n",
    "    \n",
    "    net['fc7'] = Conv2D(512, (1, 1), activation='relu',\n",
    "                               padding='same', name='fc7')(net['fc6'])\n",
    "    \n",
    "    # Block 6\n",
    "    net['conv6_1'] = Conv2D(256, (1, 1), activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv6_1')(net['fc7'])\n",
    "    net['conv6_2'] = Conv2D(512, (3, 3), strides=(2, 2),\n",
    "                                   activation='relu', padding='same',\n",
    "                                   name='conv6_2')(net['conv6_1'])\n",
    "    # Block 7\n",
    "    net['conv7_1'] = Conv2D(128, (1, 1), activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv7_1')(net['conv6_2'])\n",
    "    net['conv7_2'] = ZeroPadding2D()(net['conv7_1'])\n",
    "    net['conv7_2'] = Conv2D(256, (3, 3), strides=(2, 2),\n",
    "                                   activation='relu', padding='valid',\n",
    "                                   name='conv7_2')(net['conv7_2'])\n",
    "    \n",
    "    # Block 8\n",
    "    net['conv8_1'] = Conv2D(128, (1, 1), activation='relu',\n",
    "                                   padding='same',\n",
    "                                   name='conv8_1')(net['conv7_2'])\n",
    "    net['conv8_2'] = Conv2D(256, (3, 3), strides=(2, 2),\n",
    "                                   activation='relu', padding='same',\n",
    "                                   name='conv8_2')(net['conv8_1'])\n",
    "    net['pool6'] = GlobalAveragePooling2D(name='pool6')(net['conv8_2'])\n",
    "    \n",
    "    # Prediction from conv4_3\n",
    "    net['conv4_3_norm'] = Normalize(20, name='conv4_3_norm')(net['conv4_3'])\n",
    "    num_priors = 3\n",
    "    x = Conv2D(num_priors * 4, (3, 3), padding='same',\n",
    "                      name='conv4_3_norm_mbox_loc')(net['conv4_3_norm'])\n",
    "    net['conv4_3_norm_mbox_loc'] = x\n",
    "    flatten = Flatten(name='conv4_3_norm_mbox_loc_flat')\n",
    "    net['conv4_3_norm_mbox_loc_flat'] = flatten(net['conv4_3_norm_mbox_loc'])\n",
    "    name = 'conv4_3_norm_mbox_conf'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    x = Conv2D(num_priors * num_classes, (3, 3), padding='same',\n",
    "                      name=name)(net['conv4_3_norm'])\n",
    "    net['conv4_3_norm_mbox_conf'] = x\n",
    "    flatten = Flatten(name='conv4_3_norm_mbox_conf_flat')\n",
    "    net['conv4_3_norm_mbox_conf_flat'] = flatten(net['conv4_3_norm_mbox_conf'])\n",
    "    priorbox = PriorBox(img_size, input_shape[0] * m_scale[0], aspect_ratios=[2],\n",
    "                        variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                        name='conv4_3_norm_mbox_priorbox')\n",
    "    net['conv4_3_norm_mbox_priorbox'] = priorbox(net['conv4_3_norm'])\n",
    "    \n",
    "    # Prediction from fc7\n",
    "    num_priors = 6\n",
    "    net['fc7_mbox_loc'] = Conv2D(num_priors * 4, (3, 3),\n",
    "                                        padding='same',\n",
    "                                        name='fc7_mbox_loc')(net['fc7'])\n",
    "    flatten = Flatten(name='fc7_mbox_loc_flat')\n",
    "    net['fc7_mbox_loc_flat'] = flatten(net['fc7_mbox_loc'])\n",
    "    name = 'fc7_mbox_conf'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    net['fc7_mbox_conf'] = Conv2D(num_priors * num_classes, (3, 3),\n",
    "                                         padding='same',\n",
    "                                         name=name)(net['fc7'])\n",
    "    flatten = Flatten(name='fc7_mbox_conf_flat')\n",
    "    net['fc7_mbox_conf_flat'] = flatten(net['fc7_mbox_conf'])\n",
    "    priorbox = PriorBox(img_size, input_shape[0] * m_scale[1], max_size=input_shape[0] * m_scale[2], aspect_ratios=[2, 3],\n",
    "                        variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                        name='fc7_mbox_priorbox')\n",
    "    net['fc7_mbox_priorbox'] = priorbox(net['fc7'])\n",
    "    \n",
    "    # Prediction from conv6_2\n",
    "    num_priors = 6\n",
    "    x = Conv2D(num_priors * 4, (3, 3), padding='same',\n",
    "                      name='conv6_2_mbox_loc')(net['conv6_2'])\n",
    "    net['conv6_2_mbox_loc'] = x\n",
    "    flatten = Flatten(name='conv6_2_mbox_loc_flat')\n",
    "    net['conv6_2_mbox_loc_flat'] = flatten(net['conv6_2_mbox_loc'])\n",
    "    name = 'conv6_2_mbox_conf'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    x = Conv2D(num_priors * num_classes, (3, 3), padding='same',\n",
    "                      name=name)(net['conv6_2'])\n",
    "    net['conv6_2_mbox_conf'] = x\n",
    "    flatten = Flatten(name='conv6_2_mbox_conf_flat')\n",
    "    net['conv6_2_mbox_conf_flat'] = flatten(net['conv6_2_mbox_conf'])\n",
    "    priorbox = PriorBox(img_size, input_shape[0] * m_scale[2], max_size=input_shape[0] * m_scale[3], aspect_ratios=[2, 3],\n",
    "                        variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                        name='conv6_2_mbox_priorbox')\n",
    "    net['conv6_2_mbox_priorbox'] = priorbox(net['conv6_2'])\n",
    "    \n",
    "    # Prediction from conv7_2\n",
    "    num_priors = 6\n",
    "    x = Conv2D(num_priors * 4, (3, 3), padding='same',\n",
    "                      name='conv7_2_mbox_loc')(net['conv7_2'])\n",
    "    net['conv7_2_mbox_loc'] = x\n",
    "    flatten = Flatten(name='conv7_2_mbox_loc_flat')\n",
    "    net['conv7_2_mbox_loc_flat'] = flatten(net['conv7_2_mbox_loc'])\n",
    "    name = 'conv7_2_mbox_conf'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    x = Conv2D(num_priors * num_classes, (3, 3), padding='same',\n",
    "                      name=name)(net['conv7_2'])\n",
    "    net['conv7_2_mbox_conf'] = x\n",
    "    flatten = Flatten(name='conv7_2_mbox_conf_flat')\n",
    "    net['conv7_2_mbox_conf_flat'] = flatten(net['conv7_2_mbox_conf'])\n",
    "    priorbox = PriorBox(img_size, input_shape[0] * m_scale[3], max_size=input_shape[0] * m_scale[4], aspect_ratios=[2, 3],\n",
    "                        variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                        name='conv7_2_mbox_priorbox')\n",
    "    net['conv7_2_mbox_priorbox'] = priorbox(net['conv7_2'])\n",
    "    \n",
    "    # Prediction from conv8_2\n",
    "    num_priors = 6\n",
    "    x = Conv2D(num_priors * 4, (3, 3), padding='same',\n",
    "                      name='conv8_2_mbox_loc')(net['conv8_2'])\n",
    "    net['conv8_2_mbox_loc'] = x\n",
    "    flatten = Flatten(name='conv8_2_mbox_loc_flat')\n",
    "    net['conv8_2_mbox_loc_flat'] = flatten(net['conv8_2_mbox_loc'])\n",
    "    name = 'conv8_2_mbox_conf'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    x = Conv2D(num_priors * num_classes, (3, 3), padding='same',\n",
    "                      name=name)(net['conv8_2'])\n",
    "    net['conv8_2_mbox_conf'] = x\n",
    "    flatten = Flatten(name='conv8_2_mbox_conf_flat')\n",
    "    net['conv8_2_mbox_conf_flat'] = flatten(net['conv8_2_mbox_conf'])\n",
    "    priorbox = PriorBox(img_size, input_shape[0] * m_scale[4], max_size=input_shape[0] * m_scale[5], aspect_ratios=[2, 3],\n",
    "                        variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                        name='conv8_2_mbox_priorbox')\n",
    "    net['conv8_2_mbox_priorbox'] = priorbox(net['conv8_2'])\n",
    "    \n",
    "    # Prediction from pool6\n",
    "    num_priors = 6\n",
    "    x = Dense(num_priors * 4, name='pool6_mbox_loc_flat')(net['pool6'])\n",
    "    net['pool6_mbox_loc_flat'] = x\n",
    "    name = 'pool6_mbox_conf_flat'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    x = Dense(num_priors * num_classes, name=name)(net['pool6'])\n",
    "    net['pool6_mbox_conf_flat'] = x\n",
    "    priorbox = PriorBox(img_size, input_shape[0] * m_scale[5], max_size=input_shape[0] * m_scale[6], aspect_ratios=[2, 3],\n",
    "                        variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                        name='pool6_mbox_priorbox')\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        target_shape = (1, 1, 256)\n",
    "    else:\n",
    "        target_shape = (256, 1, 1)\n",
    "    net['pool6_reshaped'] = Reshape(target_shape,\n",
    "                                    name='pool6_reshaped')(net['pool6'])\n",
    "    net['pool6_mbox_priorbox'] = priorbox(net['pool6_reshaped'])\n",
    "    \n",
    "    # Gather all predictions\n",
    "    net_concate = [net['conv4_3_norm_mbox_loc_flat'],\n",
    "                             net['fc7_mbox_loc_flat'],\n",
    "                             net['conv6_2_mbox_loc_flat'],\n",
    "                             net['conv7_2_mbox_loc_flat'],\n",
    "                             net['conv8_2_mbox_loc_flat'],\n",
    "                             net['pool6_mbox_loc_flat']]\n",
    "    net['mbox_loc'] = Concatenate(axis=1, name='mbox_loc')(net_concate)\n",
    "    net_concate = [net['conv4_3_norm_mbox_conf_flat'],\n",
    "                              net['fc7_mbox_conf_flat'],\n",
    "                              net['conv6_2_mbox_conf_flat'],\n",
    "                              net['conv7_2_mbox_conf_flat'],\n",
    "                              net['conv8_2_mbox_conf_flat'],\n",
    "                              net['pool6_mbox_conf_flat']]\n",
    "    net['mbox_conf'] = Concatenate(axis=1, name='mbox_conf')(net_concate)\n",
    "    net_concate = [net['conv4_3_norm_mbox_priorbox'],\n",
    "                                  net['fc7_mbox_priorbox'],\n",
    "                                  net['conv6_2_mbox_priorbox'],\n",
    "                                  net['conv7_2_mbox_priorbox'],\n",
    "                                  net['conv8_2_mbox_priorbox'],\n",
    "                                  net['pool6_mbox_priorbox']]\n",
    "    net['mbox_priorbox'] = Concatenate(axis=1, name='mbox_priorbox')(net_concate)\n",
    "    \n",
    "    if hasattr(net['mbox_loc'], '_keras_shape'):\n",
    "        num_boxes = net['mbox_loc']._keras_shape[-1] // 4\n",
    "    elif hasattr(net['mbox_loc'], 'int_shape'):\n",
    "        num_boxes = K.int_shape(net['mbox_loc'])[-1] // 4\n",
    "    net['mbox_loc'] = Reshape((num_boxes, 4),\n",
    "                              name='mbox_loc_final')(net['mbox_loc'])\n",
    "    net['mbox_conf'] = Reshape((num_boxes, num_classes),\n",
    "                               name='mbox_conf_logits')(net['mbox_conf'])\n",
    "    net['mbox_conf'] = Activation('softmax',\n",
    "                                  name='mbox_conf_final')(net['mbox_conf'])\n",
    "    net_concate = [net['mbox_loc'], net['mbox_conf'], net['mbox_priorbox']]\n",
    "    net['predictions'] = Concatenate(axis=2, name='predictions')(net_concate)\n",
    "\n",
    "    model = Model(inputs=net['input'], outputs=net['predictions'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jun 21 12:43:54 2018\n",
    "\n",
    "@author: shen1994\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "class Generator(object):\n",
    "    def __init__(self, bbox_util, batch_size, image_size, \n",
    "                 train_gt, val_gt, train_keys, val_keys, \n",
    "                 saturation_var=0.5,\n",
    "                 brightness_var=0.5,\n",
    "                 contrast_var=0.5,\n",
    "                 lighting_std=0.5,\n",
    "                 hflip_prob=0.0,\n",
    "                 do_crop=True,\n",
    "                 crop_area_range=[0.75, 1.0],\n",
    "                 aspect_ratio_range=[3./4., 4./3.]):\n",
    "        self.train_gt = train_gt\n",
    "        self.val_gt = val_gt\n",
    "        self.bbox_util = bbox_util\n",
    "        self.batch_size = batch_size\n",
    "        self.train_keys = train_keys\n",
    "        self.val_keys = val_keys\n",
    "        self.train_batches = len(train_keys)\n",
    "        self.val_batches = len(val_keys)\n",
    "        self.image_size = image_size\n",
    "        self.color_jitter = []\n",
    "        if saturation_var:\n",
    "            self.saturation_var = saturation_var\n",
    "            self.color_jitter.append(self.saturation)\n",
    "        if brightness_var:\n",
    "            self.brightness_var = brightness_var\n",
    "            self.color_jitter.append(self.brightness)\n",
    "        if contrast_var:\n",
    "            self.contrast_var = contrast_var\n",
    "            self.color_jitter.append(self.contrast)\n",
    "        self.lighting_std = lighting_std\n",
    "        self.hflip_prob = hflip_prob\n",
    "        self.do_crop = do_crop\n",
    "        self.crop_area_range = crop_area_range\n",
    "        self.aspect_ratio_range = aspect_ratio_range\n",
    "    \n",
    "    def grayscale(self, rgb):\n",
    "        return rgb.dot([0.299, 0.587, 0.114])\n",
    "\n",
    "    def saturation(self, rgb):\n",
    "        gs = self.grayscale(rgb)\n",
    "        alpha = 2 * np.random.random() * self.saturation_var \n",
    "        alpha += 1 - self.saturation_var\n",
    "        rgb = rgb * alpha + (1 - alpha) * gs[:, :, None]\n",
    "        return np.clip(rgb, 0, 255)\n",
    "\n",
    "    def brightness(self, rgb):\n",
    "        alpha = 2 * np.random.random() * self.brightness_var \n",
    "        alpha += 1 - self.saturation_var\n",
    "        rgb = rgb * alpha\n",
    "        return np.clip(rgb, 0, 255)\n",
    "\n",
    "    def contrast(self, rgb):\n",
    "        gs = self.grayscale(rgb).mean() * np.ones_like(rgb)\n",
    "        alpha = 2 * np.random.random() * self.contrast_var \n",
    "        alpha += 1 - self.contrast_var\n",
    "        rgb = rgb * alpha + (1 - alpha) * gs\n",
    "        return np.clip(rgb, 0, 255)\n",
    "\n",
    "    def lighting(self, img):\n",
    "        cov = np.cov(img.reshape(-1, 3) / 255.0, rowvar=False)\n",
    "        eigval, eigvec = np.linalg.eigh(cov)\n",
    "        noise = np.random.randn(3) * self.lighting_std\n",
    "        noise = eigvec.dot(eigval * noise) * 255\n",
    "        img += noise\n",
    "        return np.clip(img, 0, 255)\n",
    "  \n",
    "    def horizontal_flip(self, img, y):\n",
    "        rand = np.random.random()\n",
    "        if  rand < self.hflip_prob and y.shape[0] > 0:\n",
    "            img = img[:, ::-1]\n",
    "            y_temp = y[:, 0]\n",
    "            y[:, 0] = [1.0] - y[:, 2]\n",
    "            y[:, 2] = [1.0] - y_temp\n",
    "\n",
    "        return img, y\n",
    " \n",
    "    def random_sized_crop(self, img, targets):\n",
    "        img_w = img.shape[1]\n",
    "        img_h = img.shape[0]\n",
    "        img_area = img_w * img_h\n",
    "        random_scale = np.random.random()\n",
    "        random_scale *= (self.crop_area_range[1] -\n",
    "                         self.crop_area_range[0])\n",
    "        random_scale += self.crop_area_range[0]\n",
    "        target_area = random_scale * img_area\n",
    "        random_ratio = np.random.random()\n",
    "        random_ratio *= (self.aspect_ratio_range[1] -\n",
    "                         self.aspect_ratio_range[0])\n",
    "        random_ratio += self.aspect_ratio_range[0]\n",
    "        w = np.round(np.sqrt(target_area * random_ratio))     \n",
    "        h = np.round(np.sqrt(target_area / random_ratio))\n",
    "        if np.random.random() < 0.5:\n",
    "            w, h = h, w\n",
    "        w = min(w, img_w)\n",
    "        w_rel = w / img_w\n",
    "        w = int(w)\n",
    "        h = min(h, img_h)\n",
    "        h_rel = h / img_h\n",
    "        h = int(h)\n",
    "        x = np.random.random() * (img_w - w)\n",
    "        x_rel = x / img_w\n",
    "        x = int(x)\n",
    "        y = np.random.random() * (img_h - h)\n",
    "        y_rel = y / img_h\n",
    "        y = int(y)\n",
    "        img = img[y:y+h, x:x+w]\n",
    "        new_targets = []\n",
    "        for box in targets:\n",
    "            cx = 0.5 * (box[0] + box[2])\n",
    "            cy = 0.5 * (box[1] + box[3])\n",
    "            if (x_rel < cx < x_rel + w_rel and\n",
    "                y_rel < cy < y_rel + h_rel):\n",
    "                xmin = (box[0] - x_rel) / w_rel\n",
    "                ymin = (box[1] - y_rel) / h_rel\n",
    "                xmax = (box[2] - x_rel) / w_rel\n",
    "                ymax = (box[3] - y_rel) / h_rel\n",
    "                xmin = max(0, xmin)\n",
    "                ymin = max(0, ymin)\n",
    "                xmax = min(1, xmax)\n",
    "                ymax = min(1, ymax)\n",
    "                box[:4] = [xmin, ymin, xmax, ymax]\n",
    "                new_targets.append(box)\n",
    "        new_targets = np.asarray(new_targets).reshape(-1, targets.shape[1])\n",
    "        return img, new_targets\n",
    "   \n",
    "    def generate(self, train=True):        \n",
    "        while True:\n",
    "            if train:\n",
    "                random.shuffle(self.train_keys)\n",
    "                keys = self.train_keys\n",
    "            else:\n",
    "                random.shuffle(self.val_keys)\n",
    "                keys = self.val_keys\n",
    "            inputs = []\n",
    "            targets = []\n",
    "            for key in keys:            \n",
    "                img_path = key\n",
    "                img = cv2.imread(img_path).astype('float32')\n",
    "                if train:\n",
    "                    y = self.train_gt[key].copy()\n",
    "                else:\n",
    "                    y = self.val_gt[key].copy()\n",
    "                if train and self.do_crop:\n",
    "                    img, y = self.random_sized_crop(img, y)\n",
    "                img = cv2.resize(img, self.image_size).astype('float32')\n",
    "                if train:\n",
    "                    random.shuffle(self.color_jitter)\n",
    "                    for jitter in self.color_jitter:\n",
    "                        img = jitter(img)\n",
    "                    if self.lighting_std:\n",
    "                        img = self.lighting(img)\n",
    "                    if self.hflip_prob > 0:\n",
    "                        img, y = self.horizontal_flip(img, y)\n",
    "                y = self.bbox_util.assign_boxes(y)\n",
    "                inputs.append(img)                \n",
    "                targets.append(y)\n",
    "                if len(targets) == self.batch_size:\n",
    "                    tmp_inp = np.array(inputs)\n",
    "                    tmp_targets = np.array(targets)\n",
    "                    inputs = []\n",
    "                    targets = []\n",
    "                    yield preprocess_input(tmp_inp), tmp_targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SSD training utils.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MultiboxLoss(object):\n",
    "    \"\"\"Multibox loss with some helper functions.\n",
    "    # Arguments\n",
    "        num_classes: Number of classes including background.\n",
    "        alpha: Weight of L1-smooth loss.\n",
    "        neg_pos_ratio: Max ratio of negative to positive boxes in loss.\n",
    "        background_label_id: Id of background label.\n",
    "        negatives_for_hard: Number of negative boxes to consider\n",
    "            it there is no positive boxes in batch.\n",
    "    # References\n",
    "        https://arxiv.org/abs/1512.02325\n",
    "    # TODO\n",
    "        Add possibility for background label id be not zero\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, alpha=1.0, neg_pos_ratio=3.0,\n",
    "                 background_label_id=0, negatives_for_hard=100.0):\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        if background_label_id != 0:\n",
    "            raise Exception('Only 0 as background label id is supported')\n",
    "        self.background_label_id = background_label_id\n",
    "        self.negatives_for_hard = negatives_for_hard\n",
    "\n",
    "    def _l1_smooth_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute L1-smooth loss.\n",
    "        # Arguments\n",
    "            y_true: Ground truth bounding boxes,\n",
    "                tensor of shape (?, num_boxes, 4).\n",
    "            y_pred: Predicted bounding boxes,\n",
    "                tensor of shape (?, num_boxes, 4).\n",
    "        # Returns\n",
    "            l1_loss: L1-smooth loss, tensor of shape (?, num_boxes).\n",
    "        # References\n",
    "            https://arxiv.org/abs/1504.08083\n",
    "        \"\"\"\n",
    "        abs_loss = tf.abs(y_true - y_pred)\n",
    "        sq_loss = 0.5 * (y_true - y_pred)**2\n",
    "        l1_loss = tf.where(tf.less(abs_loss, 1.0), sq_loss, abs_loss - 0.5)\n",
    "        return tf.reduce_sum(l1_loss, -1)\n",
    "\n",
    "    def _softmax_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute softmax loss.\n",
    "        # Arguments\n",
    "            y_true: Ground truth targets,\n",
    "                tensor of shape (?, num_boxes, num_classes).\n",
    "            y_pred: Predicted logits,\n",
    "                tensor of shape (?, num_boxes, num_classes).\n",
    "        # Returns\n",
    "            softmax_loss: Softmax loss, tensor of shape (?, num_boxes).\n",
    "        \"\"\"\n",
    "        y_pred = tf.maximum(tf.minimum(y_pred, 1 - 1e-15), 1e-15)\n",
    "        softmax_loss = -tf.reduce_sum(y_true * tf.log(y_pred),\n",
    "                                      axis=-1)\n",
    "        return softmax_loss\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute mutlibox loss.\n",
    "        # Arguments\n",
    "            y_true: Ground truth targets,\n",
    "                tensor of shape (?, num_boxes, 4 + num_classes + 8),\n",
    "                priors in ground truth are fictitious,\n",
    "                y_true[:, :, -8] has 1 if prior should be penalized\n",
    "                    or in other words is assigned to some ground truth box,\n",
    "                y_true[:, :, -7:] are all 0.\n",
    "            y_pred: Predicted logits,\n",
    "                tensor of shape (?, num_boxes, 4 + num_classes + 8).\n",
    "        # Returns\n",
    "            loss: Loss for prediction, tensor of shape (?,).\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(y_true)[0]\n",
    "        num_boxes = tf.to_float(tf.shape(y_true)[1])\n",
    "\n",
    "        # loss for all priors\n",
    "        conf_loss = self._softmax_loss(y_true[:, :, 4:-8],\n",
    "                                       y_pred[:, :, 4:-8])\n",
    "        loc_loss = self._l1_smooth_loss(y_true[:, :, :4],\n",
    "                                        y_pred[:, :, :4])\n",
    "\n",
    "        # get positives loss\n",
    "        num_pos = tf.reduce_sum(y_true[:, :, -8], axis=-1)\n",
    "        pos_loc_loss = tf.reduce_sum(loc_loss * y_true[:, :, -8],\n",
    "                                     axis=1)\n",
    "        pos_conf_loss = tf.reduce_sum(conf_loss * y_true[:, :, -8],\n",
    "                                      axis=1)\n",
    "\n",
    "        # get negatives loss, we penalize only confidence here\n",
    "        num_neg = tf.minimum(self.neg_pos_ratio * num_pos,\n",
    "                             num_boxes - num_pos)\n",
    "        pos_num_neg_mask = tf.greater(num_neg, 0)\n",
    "        has_min = tf.to_float(tf.reduce_any(pos_num_neg_mask))\n",
    "        num_neg = tf.concat(axis=0, values=[num_neg,\n",
    "                                [(1 - has_min) * self.negatives_for_hard]])\n",
    "        num_neg_batch = tf.reduce_min(tf.boolean_mask(num_neg,\n",
    "                                                      tf.greater(num_neg, 0)))\n",
    "        num_neg_batch = tf.to_int32(num_neg_batch)\n",
    "        confs_start = 4 + self.background_label_id + 1\n",
    "        confs_end = confs_start + self.num_classes - 1\n",
    "        max_confs = tf.reduce_max(y_pred[:, :, confs_start:confs_end],\n",
    "                                  axis=2)\n",
    "        _, indices = tf.nn.top_k(max_confs * (1 - y_true[:, :, -8]),\n",
    "                                 k=num_neg_batch)\n",
    "        batch_idx = tf.expand_dims(tf.range(0, batch_size), 1)\n",
    "        batch_idx = tf.tile(batch_idx, (1, num_neg_batch))\n",
    "        full_indices = (tf.reshape(batch_idx, [-1]) * tf.to_int32(num_boxes) +\n",
    "                        tf.reshape(indices, [-1]))\n",
    "        # full_indices = tf.concat(2, [tf.expand_dims(batch_idx, 2),\n",
    "        #                              tf.expand_dims(indices, 2)])\n",
    "        # neg_conf_loss = tf.gather_nd(conf_loss, full_indices)\n",
    "        neg_conf_loss = tf.gather(tf.reshape(conf_loss, [-1]),\n",
    "                                  full_indices)\n",
    "        neg_conf_loss = tf.reshape(neg_conf_loss,\n",
    "                                   [batch_size, num_neg_batch])\n",
    "        neg_conf_loss = tf.reduce_sum(neg_conf_loss, axis=1)\n",
    "\n",
    "        # loss is sum of positives and negatives\n",
    "        total_loss = pos_conf_loss + neg_conf_loss\n",
    "        total_loss /= (num_pos + tf.to_float(num_neg_batch))\n",
    "        num_pos = tf.where(tf.not_equal(num_pos, 0), num_pos,\n",
    "                            tf.ones_like(num_pos))\n",
    "        total_loss += (self.alpha * pos_loc_loss) / num_pos\n",
    "        return total_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ssd utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"conv4_3_norm\" (type Normalize).\n\nin user code:\n\n    File \"C:\\Users\\Harry Parker\\AppData\\Local\\Temp\\ipykernel_31160\\1110528589.py\", line 47, in call  *\n        output *= self.gamma\n\n    ValueError: Dimensions must be equal, but are 256 and 38 for '{{node conv4_3_norm/mul}} = Mul[T=DT_FLOAT](conv4_3_norm/l2_normalize, conv4_3_norm/mul/ReadVariableOp)' with input shapes: [?,38,38,256], [38].\n\n\nCall arguments received by layer \"conv4_3_norm\" (type Normalize):\n  • x=tf.Tensor(shape=(None, 38, 38, 256), dtype=float32)\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m decay \u001b[39m=\u001b[39m \u001b[39m0.9\u001b[39m\n\u001b[0;32m     66\u001b[0m optimizer \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mbase_lr, beta_1\u001b[39m=\u001b[39mdecay, beta_2\u001b[39m=\u001b[39m\u001b[39m0.999\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m model \u001b[39m=\u001b[39m SSD300(input_shape\u001b[39m=\u001b[39;49minput_shape, num_classes\u001b[39m=\u001b[39;49mNUM_CLASSES)\n\u001b[0;32m     68\u001b[0m model\u001b[39m.\u001b[39mload_weights(\u001b[39m'\u001b[39m\u001b[39mmodel/weights.30-1.81.hdf5\u001b[39m\u001b[39m'\u001b[39m, by_name\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     69\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39mMultiboxLoss(NUM_CLASSES, neg_pos_ratio\u001b[39m=\u001b[39m\u001b[39m2.0\u001b[39m)\u001b[39m.\u001b[39mcompute_loss)\n",
      "Cell \u001b[1;32mIn[18], line 135\u001b[0m, in \u001b[0;36mSSD300\u001b[1;34m(input_shape, num_classes)\u001b[0m\n\u001b[0;32m    132\u001b[0m net[\u001b[39m'\u001b[39m\u001b[39mpool6\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m GlobalAveragePooling2D(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpool6\u001b[39m\u001b[39m'\u001b[39m)(net[\u001b[39m'\u001b[39m\u001b[39mconv8_2\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    134\u001b[0m \u001b[39m# Prediction from conv4_3\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m net[\u001b[39m'\u001b[39m\u001b[39mconv4_3_norm\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m Normalize(\u001b[39m20\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mconv4_3_norm\u001b[39;49m\u001b[39m'\u001b[39;49m)(net[\u001b[39m'\u001b[39;49m\u001b[39mconv4_3\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    136\u001b[0m num_priors \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m    137\u001b[0m x \u001b[39m=\u001b[39m Conv2D(num_priors \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    138\u001b[0m                   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mconv4_3_norm_mbox_loc\u001b[39m\u001b[39m'\u001b[39m)(net[\u001b[39m'\u001b[39m\u001b[39mconv4_3_norm\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\HARRYP~1\\AppData\\Local\\Temp\\__autograph_generated_fileo0a6k7o3.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     10\u001b[0m output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(K)\u001b[39m.\u001b[39ml2_normalize, (ag__\u001b[39m.\u001b[39mld(x), ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39maxis), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(output)\n\u001b[1;32m---> 12\u001b[0m output \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"conv4_3_norm\" (type Normalize).\n\nin user code:\n\n    File \"C:\\Users\\Harry Parker\\AppData\\Local\\Temp\\ipykernel_31160\\1110528589.py\", line 47, in call  *\n        output *= self.gamma\n\n    ValueError: Dimensions must be equal, but are 256 and 38 for '{{node conv4_3_norm/mul}} = Mul[T=DT_FLOAT](conv4_3_norm/l2_normalize, conv4_3_norm/mul/ReadVariableOp)' with input shapes: [?,38,38,256], [38].\n\n\nCall arguments received by layer \"conv4_3_norm\" (type Normalize):\n  • x=tf.Tensor(shape=(None, 38, 38, 256), dtype=float32)\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jun 20 10:05:12 2018\n",
    "@author: shen1994\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import keras\n",
    "import pickle\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def concatenate_ssd300_boxes():\n",
    "    \n",
    "    prior_boxes_list = []\n",
    "    images_path = 'images'\n",
    "    prior_boxes_path = []\n",
    "    for file_name in os.listdir(images_path):\n",
    "        if len(file_name) > 8 and operator.eq(file_name[0:8], 'priorbox'):\n",
    "            prior_boxes_path.append(file_name)\n",
    "      \n",
    "    prior_boxes_number = []\n",
    "    for elem in prior_boxes_path:\n",
    "        flag = True\n",
    "        k = ''\n",
    "        for i in range(len(elem)):\n",
    "            \n",
    "            if elem[i] == '_' and flag:\n",
    "                flag = False\n",
    "                continue\n",
    "            if elem[i] == '_' and not flag:\n",
    "                break\n",
    "            if not flag:\n",
    "                k += elem[i]\n",
    "\n",
    "        prior_boxes_number.append(int(k))\n",
    "        \n",
    "    prior_boxes_index = np.argsort(-np.array(prior_boxes_number))\n",
    "    \n",
    "    for index in range(len(prior_boxes_path)):\n",
    "        true_index = prior_boxes_index[index]\n",
    "        full_path = images_path + os.sep + prior_boxes_path[true_index]\n",
    "        prior_boxes_list.append(pickle.load(open(full_path, 'rb')))\n",
    "            \n",
    "    prior_boxes = np.concatenate(prior_boxes_list, axis=0)\n",
    "        \n",
    "    return prior_boxes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "    \n",
    "    model_path = 'model'\n",
    "    if not os.path.exists(model_path):\n",
    "        os.mkdir(model_path)\n",
    "    \n",
    "    NUM_CLASSES = 2\n",
    "    input_shape = [300, 300, 3]\n",
    "    \n",
    "    batch_size = 8\n",
    "    epochs = 30\n",
    "    base_lr = 1e-5\n",
    "    decay = 0.9\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=base_lr, beta_1=decay, beta_2=0.999)\n",
    "    model = SSD300(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "    model.load_weights('model/weights.30-1.81.hdf5', by_name=True)\n",
    "    model.compile(optimizer=optimizer, loss=MultiboxLoss(NUM_CLASSES, neg_pos_ratio=2.0).compute_loss)\n",
    "    priors = concatenate_ssd300_boxes()\n",
    "    bbox_util = BBoxUtility(NUM_CLASSES, priors)\n",
    "    train_bbx_gt = pickle.load(open('images/train_bbx_gt.pkl', 'rb'))\n",
    "    valid_bbx_gt = pickle.load(open('images/valid_bbx_gt.pkl', 'rb'))\n",
    "    train_keys = list(train_bbx_gt.keys())\n",
    "    valid_keys = list(valid_bbx_gt.keys())\n",
    "\n",
    "    gen = Generator(bbox_util, batch_size, (input_shape[0], input_shape[1]),\n",
    "                    train_bbx_gt, valid_bbx_gt, train_keys, valid_keys, do_crop=False)\n",
    "    \n",
    "    def schedule(epoch, decay=decay):\n",
    "        return base_lr #  * decay ** (epoch)\n",
    "    callbacks = [keras.callbacks.ModelCheckpoint('model/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                                 verbose=1,\n",
    "                                                 save_weights_only=True),\n",
    "                 keras.callbacks.LearningRateScheduler(schedule)]\n",
    "\n",
    "    history = model.fit_generator(generator=gen.generate(True), \n",
    "                                  steps_per_epoch = int(gen.train_batches / gen.batch_size),\n",
    "                                  epochs=epochs,\n",
    "                                  verbose=1,\n",
    "                                  callbacks=callbacks,\n",
    "                                  validation_data=gen.generate(False),\n",
    "                                  validation_steps=int(gen.val_batches / gen.batch_size),\n",
    "                                  workers=1)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "from tensorflow.python.framework import graph_io\n",
    "\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a prunned computation graph.\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    prunned so subgraphs that are not neccesary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(session, input_graph_def,\n",
    "                                                      output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "    \n",
    "    NUM_CLASSES = 2\n",
    "    input_shape = [300, 300, 3]\n",
    "\n",
    "    model = SSD300(input_shape=input_shape, num_classes=NUM_CLASSES)\n",
    "    model.load_weights('model/weights.04-1.72.hdf5', by_name=True)\n",
    "\n",
    "    # boxes = tf.placeholder(dtype='float32', shape=(None, 4))\n",
    "    # scores = tf.placeholder(dtype='float32', shape=(None,))\n",
    "    # nms = tf.image.non_max_suppression(boxes, scores, 100, iou_threshold=0.45)\n",
    "    print('input name is: ', model.input.name)\n",
    "    print('output name is: ', model.output.name)\n",
    "    \n",
    "    K.set_learning_phase(0)\n",
    "    frozen_graph = freeze_session(K.get_session(), output_names=[model.output.op.name])\n",
    "    graph_io.write_graph(frozen_graph, \"model/\", \"pico_FaceDetect_model.pb\", as_text=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b503a1eefdb65dd58b69f1b47019840af2c24b3838e9a0722caae24111689e23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
