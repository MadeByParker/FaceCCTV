{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup and Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --user labelme tensorflow tensorflow-gpu opencv-python matplotlib albumentations split-folders imutils scikit-learn keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Review Dataset and Build Image Loading Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import Tensorflow and Dependences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Limit GPU Memory Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load Image into TF Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '0--Parade/0_Parade_marchingband_1_849.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# iterate through each folder path\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m folder_path \u001b[39min\u001b[39;00m folder_paths:\n\u001b[0;32m     14\u001b[0m     \u001b[39m# get the list of images in the current folder\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m       \u001b[39mfor\u001b[39;00m image_file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(folder_path):\n\u001b[0;32m     16\u001b[0m             \u001b[39mif\u001b[39;00m image_file\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     17\u001b[0m                   image_paths\u001b[39m.\u001b[39mappend(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mWIDER_train\u001b[39m\u001b[39m'\u001b[39m, folder_path, image_file))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '0--Parade/0_Parade_marchingband_1_849.jpg'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import scipy.io\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models.config import rfcn_config as cfg\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# https://www.cs.virginia.edu/~vicente/recognition/notebooks/image_processing_lab.html\n",
    "class WiderFaceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_path, metadata_path, object_min_dim=300, transform=None):\n",
    "        self.image_path = image_path\n",
    "        self.metadata_path = metadata_path\n",
    "        self.transform = transform\n",
    "        self.pil2tensor = transforms.ToTensor()\n",
    "\n",
    "        self.convert_to_image_list(self.metadata_path, object_min_dim)\n",
    "\n",
    "    def convert_to_image_list(self, path, object_min_dim):\n",
    "        self.f = scipy.io.loadmat(path)\n",
    "        self.event_list = self.f.get('event_list')\n",
    "        self.file_list = self.f.get('file_list')\n",
    "        self.face_bbx_list = self.f.get('face_bbx_list')\n",
    "        self.occlusion_label_list = self.f.get('occlusion_label_list')\n",
    "        self.pose_label_list = self.f.get('pose_label_list')\n",
    "\n",
    "        image_metadata = {\n",
    "            'image_location': [],\n",
    "            'image_ground_truth': []\n",
    "        }\n",
    "\n",
    "        for idx, event in enumerate(self.event_list):\n",
    "            event = event[0][0]\n",
    "\n",
    "            for file_idx, file_path in enumerate(self.file_list[idx][0]):\n",
    "                file_name = file_path[0][0] + '.jpg'\n",
    "                file_name = event + '/' + file_name\n",
    "                file_path = os.path.abspath(self.image_path + '/' + file_name)\n",
    "                \n",
    "                bounding_boxes = self.face_bbx_list[idx][0][file_idx][0]\n",
    "                occlusions = self.occlusion_label_list[idx][0][file_idx][0]\n",
    "                pose = self.pose_label_list[idx][0][file_idx][0]\n",
    "\n",
    "                # Filter out medium and hard bounding_boxes if any \n",
    "                bounding_boxes_filtered = []\n",
    "                for occlusion_idx, occlusion in enumerate(occlusions):\n",
    "                    if occlusion[0] == 0 and pose[occlusion_idx][0] == 0:\n",
    "                        # Now, get only large Bboxes\n",
    "                        #print(bounding_boxes[occlusion_idx])\n",
    "                        if bounding_boxes[occlusion_idx][2] > object_min_dim and bounding_boxes[occlusion_idx][3] > object_min_dim:\n",
    "                            bounding_boxes_filtered.append(bounding_boxes[occlusion_idx])\n",
    "                \n",
    "                \n",
    "                if len(bounding_boxes_filtered) > 0:\n",
    "                    image_metadata['image_location'].append(file_path)\n",
    "                    image_metadata['image_ground_truth'].append(bounding_boxes_filtered)\n",
    "\n",
    "                # Plot anchors\n",
    "                # positive_anchors, negative_anchors = get_regions(np.array(Image.open(file_path).convert('RGB'), dtype=np.uint32), 100, bounding_boxes_filtered)\n",
    "                # print(\"positive:\",len(positive_anchors))\n",
    "                # print(\"negative:\",len(negative_anchors))\n",
    "                # self.plot_boxes(file_path, positive_anchors, [], bounding_boxes_filtered)\n",
    "\n",
    "                # break\n",
    "            # break\n",
    "\n",
    "        # convert to pandas\n",
    "        self.dataset = pd.DataFrame.from_dict(image_metadata)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            with open(self.dataset.iloc[idx]['image_location'], 'rb') as f:\n",
    "            #with open(\"/home/vedvalsangkar/Face-R-FCN/data/widerface/WIDER_train/images/0--Parade/0_Parade_Parade_0_452.jpg\", 'rb') as f:\n",
    "                image = Image.open(f)\n",
    "                image = image.convert('RGB')\n",
    "                #print(image.size)\n",
    "                image, boxes = self.resize_image(image, self.dataset.iloc[idx]['image_ground_truth'],\n",
    "                                                 dimension=cfg.IMAGE_INPUT_DIMS)\n",
    "\n",
    "        except Exception:\n",
    "            print(\"Image not found..\", traceback.format_exception())\n",
    "            return ([], self.dataset.iloc[idx]['image_ground_truth'])\n",
    "\n",
    "        image_tensor = self.pil2tensor(image)\n",
    "        # ground_truth_tensor = boxes\n",
    "        ground_truth_tensor = torch.tensor(np.array(boxes))\n",
    "\n",
    "        return (image_tensor, ground_truth_tensor, self.dataset.iloc[idx]['image_location'])\n",
    "\n",
    "    # Referenced from: https://jdhao.github.io/2017/11/06/resize-image-to-square-with-padding/\n",
    "    def resize_image(self, im, b_boxes, dimension=cfg.IMAGE_INPUT_DIMS):\n",
    "        b_boxes = np.array(b_boxes, dtype=np.float)\n",
    "\n",
    "        old_size = im.size\n",
    "        \n",
    "        ratio = float(dimension) / max(old_size)\n",
    "        new_size = tuple([int(x * ratio) for x in old_size])\n",
    "\n",
    "        im = im.resize(new_size, Image.ANTIALIAS)\n",
    "\n",
    "        offset_x = (dimension - new_size[0]) // 2\n",
    "        offset_y = (dimension - new_size[1]) // 2\n",
    "\n",
    "        # create a new image and paste the resized on it\n",
    "        new_im = Image.new(\"RGB\", (dimension, dimension))\n",
    "        new_im.paste(im, (offset_x, offset_y))\n",
    "\n",
    "        # Re-size and offset bounding boxes based on image\n",
    "        b_boxes[:,0] = (b_boxes[:,0] * ratio) + offset_x\n",
    "        b_boxes[:,1] = (b_boxes[:,1] * ratio) + offset_y\n",
    "        \n",
    "        b_boxes[:,0] = np.clip(b_boxes[:,0], 0, None)\n",
    "        b_boxes[:,1] = np.clip(b_boxes[:,1], 0, None)\n",
    "\n",
    "        b_boxes[:,2] = np.clip(b_boxes[:,2],1, None) * ratio\n",
    "        b_boxes[:,3] = np.clip(b_boxes[:,3],1, None) * ratio\n",
    "\n",
    "        \"\"\"for box in b_boxes:\n",
    "            x = int(abs(box[0] * ratio + offset_x))\n",
    "            y = int(abs(box[1] * ratio + offset_y))\n",
    "            l = int(box[2] * ratio)\n",
    "            b = int(box[3] * ratio)\n",
    "            new_box = [x, y, l, b]\n",
    "            results.append(new_box)\n",
    "        \"\"\"\n",
    "        return new_im, b_boxes[:60,:]\n",
    "\n",
    "    def plot_boxes(self, file, positive_anchors, negative_anchors, boxes):\n",
    "        im = np.array(Image.open(file).convert('RGB'), dtype=np.uint8)\n",
    "\n",
    "        # Create figure and axes\n",
    "        fig, ax = plt.subplots(1)\n",
    "\n",
    "        # Display the image\n",
    "        ax.imshow(im)\n",
    "\n",
    "        if positive_anchors:\n",
    "            for i, box in enumerate(positive_anchors):\n",
    "                rect = patches.Rectangle((box[0], box[1]), box[2], box[3], linewidth=1, edgecolor='green',\n",
    "                                         facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "        if negative_anchors:\n",
    "            for i, box in enumerate(negative_anchors):\n",
    "                rect = patches.Rectangle((box[0], box[1]), box[2], box[3], linewidth=1, edgecolor='red',\n",
    "                                         facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "        for i, box in enumerate(boxes):\n",
    "            rect = patches.Rectangle((box[0], box[1]), box[2], box[3], linewidth=1, edgecolor='blue', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def get_regions(features, N, list_bb):\n",
    "    \"\"\"\n",
    "    Function to generate Positive and Negative anchors.\n",
    "    :param features:  Extracted features\n",
    "    :param N:         Number of bounding boxes\n",
    "    :param list_bb:   Bounding boxes (scaled to current configuration)\n",
    "    :return:          Positive and negative anchors.\n",
    "    \"\"\"\n",
    "\n",
    "    # box_size = [(32, 32), (64, 32), (32, 64)]\n",
    "    box_size = [(128, 128), (256, 128), (128, 256), (256, 256), (256, 512), (512, 256)]\n",
    "    \"\"\"Sizes for region proposals\"\"\"\n",
    "\n",
    "    feat_h, feat_w, _ = features.shape\n",
    "\n",
    "    print(\"Shape of features:\", features.shape)\n",
    "    print(\"Shape of bounding box:\", list_bb[0])\n",
    "\n",
    "    pos_anc = []\n",
    "    neg_anc = []\n",
    "    scale = 1\n",
    "\n",
    "    for bs in box_size:\n",
    "        for i in range(0, feat_h):\n",
    "            for j in range(0, feat_w):\n",
    "\n",
    "                max_iou = 0\n",
    "\n",
    "                x = min(feat_h, max(0, (i - (bs[0] // 2))))\n",
    "                y = min(feat_w, max(0, (j - (bs[1] // 2))))\n",
    "\n",
    "                # im_slice = features[x:x + bs[0], y:y + bs[1]]\n",
    "                frame_a = (x, y, x + bs[0], y + bs[1])\n",
    "                \"\"\"\n",
    "                Frame A is the sliding window for calculation.\n",
    "                \"\"\"\n",
    "\n",
    "                for bb in list_bb:\n",
    "                    frame_b = (bb[1], bb[0], bb[1] + bb[3], bb[0] + bb[2])\n",
    "                    \"\"\"\n",
    "                    Frame B is the input \"\"\"\n",
    "                    iou = calc_IOU(frame_a, frame_b)\n",
    "                    if max_iou < iou:\n",
    "                        max_iou = iou\n",
    "\n",
    "                    # print(max_iou)\n",
    "\n",
    "                if max_iou > 0.762:\n",
    "                    print(frame_a, frame_b, max_iou)\n",
    "                    # pos_anc.append((x * scale, (x + bs[0]) * scale, y * scale, (y + bs[1]) * scale))\n",
    "                    pos_anc.append([y * scale, x * scale, bs[1], bs[0]])\n",
    "                elif max_iou < 0.05:\n",
    "                    # neg_anc.append((x * scale, (x + bs[0]) * scale, y * scale, (y + bs[1]) * scale))\n",
    "                    neg_anc.append([y * scale, x * scale, bs[1], bs[0]])\n",
    "\n",
    "    return pos_anc, neg_anc\n",
    "\n",
    "\n",
    "def calc_IOU(boxA, boxB):\n",
    "    \"\"\"\n",
    "    Function taken from this site:\n",
    "    https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n",
    "    :param boxA: (X_min, Y_min, X_max, Y_max)\n",
    "    :param boxB: (X_min, Y_min, X_max, Y_max)\n",
    "    :return: IOU area ratio.\n",
    "    \"\"\"\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = tf.data.Dataset.list_files('data\\\\img\\\\img\\\\*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(x): \n",
    "    byte_img = tf.io.read_file(x)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.map(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 View Raw Images with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = train_images.batch(4).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images = next(image_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx, image in enumerate(plot_images):\n",
    "    ax[idx].imshow(image) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Partition Unargumented Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Split data into training and testing and valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio('./data/img', output=\"./data/ttvimg\", seed=1337, ratio=(.8, 0.1,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Move the Matching Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in ['train','test','val']:\n",
    "    for file in os.listdir(os.path.join('data', folder, 'images')):\n",
    "        \n",
    "        filename = file.split('.')[0]+'.json'\n",
    "        existing_filepath = os.path.join('data','labels', filename)\n",
    "        if os.path.exists(existing_filepath): \n",
    "            new_filepath = os.path.join('data',folder,'labels',filename)\n",
    "            os.replace(existing_filepath, new_filepath)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Setup Albumentations Transform Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as alb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([alb.RandomCrop(width=1280, height=720)], bbox_params=alb.BboxParams(format='albumentations', label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Load a Test Image and Annotation with OpenCV and JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testImage = cv2.imread(os.path.join('data', 'WIDERFace', 'train','0--Parade','0_Parade_marchingband_1_5.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'train', 'labels', '1.json'), 'r') as f:\n",
    "    label = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label['shapes'][1]['points']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Extract Coordinates and Rescale to Match Image Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = [0,0,0,0]\n",
    "coords[0] = label['shapes'][1]['points'][0][0]\n",
    "coords[1] = label['shapes'][1]['points'][0][1]\n",
    "coords[2] = label['shapes'][1]['points'][1][0]\n",
    "coords[3] = label['shapes'][1]['points'][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = list(np.divide(coords, [1920,1080,1920,1080]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Apply Augmentations and View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = augmentor(image=testImage, bboxes=[coords], class_labels=['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['bboxes'][0][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(augmented['image'], \n",
    "              tuple(np.multiply(augmented['bboxes'][0][:2], [1280, 720]).astype(int)),\n",
    "              tuple(np.multiply(augmented['bboxes'][0][2:], [1280, 720]).astype(int)), \n",
    "                    (255,0,0), 2)\n",
    "\n",
    "plt.imshow(augmented['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Build and Run Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Run Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in ['train']: \n",
    "    for image in os.listdir(os.path.join('data', partition, 'images')):\n",
    "        img = cv2.imread(os.path.join('data', partition, 'images', image))\n",
    "\n",
    "        coords = [0,0,0.00001,0.00001]\n",
    "        label_path = os.path.join('data', partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "\n",
    "            coords[0] = label['shapes'][0]['points'][0][0]\n",
    "            coords[1] = label['shapes'][0]['points'][0][1]\n",
    "            coords[2] = label['shapes'][0]['points'][1][0]\n",
    "            coords[3] = label['shapes'][0]['points'][1][1]\n",
    "            coords = list(np.divide(coords, [1920,1080,1920,1080]))\n",
    "\n",
    "        try: \n",
    "            for x in range(60):\n",
    "                augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n",
    "                cv2.imwrite(os.path.join('augmented-data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "\n",
    "                annotation = {}\n",
    "                annotation['image'] = image\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    if len(augmented['bboxes']) == 0: \n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0 \n",
    "                    else: \n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "                else: \n",
    "                    annotation['bbox'] = [0,0,0,0]\n",
    "                    annotation['class'] = 0 \n",
    "\n",
    "\n",
    "                with open(os.path.join('augmented-data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Load Augmented Images to Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.data.Dataset.list_files('augmented-data\\\\train\\\\images\\\\*.jpg', shuffle=False)\n",
    "train_images = train_images.map(load_image)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (240,240)))\n",
    "train_images = train_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('augmented-data\\\\test\\\\images\\\\*.jpg', shuffle=False)\n",
    "test_images = test_images.map(load_image)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (240,240)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('augmented-data\\\\val\\\\images\\\\*.jpg', shuffle=False)\n",
    "val_images = val_images.map(load_image)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (240,240)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.as_numpy_iterator().next()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Prepare Labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Build Label Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding = \"utf-8\") as f:\n",
    "        label = json.load(f)\n",
    "        \n",
    "    return [label['class']], label['bbox']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Load Labels to Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files('augmented-data\\\\train\\\\labels\\\\*.json', shuffle=False)\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))\n",
    "\n",
    "# Load the annotations file\n",
    "train_annotations_path = 'data\\\\wider_face_split\\\\wider_face_train_bbx_gt.txt'\n",
    "train_annotations = []\n",
    "with open(train_annotations_path, 'r') as file:\n",
    "    for line in file:\n",
    "        train_annotations.append(list(map(int, line.strip().split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotations file\n",
    "train_annotations_path = 'data\\\\wider_face_split\\\\wider_face_train_bbx_gt.txt'\n",
    "train_annotations = []\n",
    "with open(train_annotations_path, 'r') as file:\n",
    "    for line in file:\n",
    "        train_annotations.append(list(map(int, line.strip().split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotations file\n",
    "val_annotations_path = 'data\\\\wider_face_split\\\\wider_face_train_bbx_gt.txt'\n",
    "val_annotations = []\n",
    "with open(val_annotations_path, 'r') as file:\n",
    "    for line in file:\n",
    "        val_annotations.append(list(map(int, line.strip().split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Combine Labels and Image Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Check Partition Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images), len(test_images), len(val_images), len(train_annotations), len(test_annotations), len(val_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Final Datasets (Images/Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train = train.shuffle(5000)\n",
    "train = train.batch(8)\n",
    "train = train.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test = test.shuffle(1300)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val = val.shuffle(1000)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 View Images and Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = train.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = data_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(40,40))\n",
    "for idx in range(4): \n",
    "    sample_image = res[0][idx]\n",
    "    sample_coords = res[1][1][idx]\n",
    "    \n",
    "    cv2.rectangle(sample_image, \n",
    "                  tuple(np.multiply(sample_coords[:2], [240,240]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:], [240,240]).astype(int)), \n",
    "                        (255,0,0), 2)\n",
    "\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Build Deep Learning Model using the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Import Layers and Base Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Download VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Build Instance of Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    input_layer = Input(shape=(240,240,3))\n",
    "    \n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "\n",
    "    # Classification Model  \n",
    "    f1 = GlobalMaxPooling2D()(vgg)\n",
    "    class1 = Dense(2048, activation='relu')(f1)\n",
    "    class2 = Dense(1, activation='sigmoid')(class1)\n",
    "    \n",
    "    # Bounding box model\n",
    "    f2 = GlobalMaxPooling2D()(vgg)\n",
    "    regress1 = Dense(2048, activation='relu')(f2)\n",
    "    regress2 = Dense(4, activation='sigmoid')(regress1)\n",
    "    \n",
    "    facedetector = Model(inputs=input_layer, outputs=[class2, regress2])\n",
    "    return facedetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Test out Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facedetector = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facedetector.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords = facedetector.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Define Losses and Optimisers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Define Optimiser and LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = len(train)\n",
    "lr_decay = (1./0.75-1)/batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Create Localisation Loss and Classification Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localisation_loss(y_true, y_pred):\n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - y_pred[:,:2]))\n",
    "\n",
    "    h_true = y_true[:,3] - y_true[:,1]\n",
    "    w_true = y_true[:,2] - y_true[:,0]\n",
    "\n",
    "    h_pred = y_pred[:,3] - y_pred[:,1]\n",
    "    w_pred = y_pred[:,2] - y_pred[:,0]\n",
    "\n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true - h_pred))\n",
    "\n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss = tf.keras.losses.BinaryCrossentropy()\n",
    "regressloss = localisation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Test out Loss Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localisation_loss(y[1], coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss(y[0], classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressloss(y[1], coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Train Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Create Custom Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTracker(Model):\n",
    "    def __init__(self, facecctv, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = facecctv\n",
    "\n",
    "    def compile(self, classloss, localisation_loss, opt, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.clsloss = classloss\n",
    "        self.localloss = localisation_loss\n",
    "        self.opt = opt\n",
    "\n",
    "    def train_step(self, batch, **kwargs):\n",
    "\n",
    "        X, y = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            classes, coords = self.model(X, training=True)\n",
    "\n",
    "            batch_class_loss = self.clsloss(y[0], classes)\n",
    "            batch_localisation_loss = self.localloss(tf.cast(y[1], tf.float32), coords)\n",
    "\n",
    "            total_loss = (0.5*batch_class_loss) + batch_localisation_loss\n",
    "\n",
    "            gradient = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        \n",
    "        self.opt.apply_gradients(zip(gradient, self.model.trainable_variables))\n",
    "\n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_class_loss, \"regress_loss\":batch_localisation_loss}\n",
    "\n",
    "    def test_step(self, batch, **kwargs):\n",
    "        X, y = batch\n",
    "\n",
    "        classes, coords = self.model(X, training=False)\n",
    "\n",
    "        batch_class_loss = self.clsloss(y[0], classes)\n",
    "        batch_localisation_loss = self.localloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss = (0.5*batch_class_loss) + batch_localisation_loss\n",
    "\n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_class_loss, \"regress_loss\":batch_localisation_loss}\n",
    "\n",
    "    def call(self, X, **kwargs): \n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceTracker(facedetector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(classloss, localisation_loss, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train, epochs=10, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
    "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Make Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = facedetector.predict(test_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(40,40))\n",
    "for idx in range(4): \n",
    "    sample_image = test_sample[0][idx]\n",
    "    sample_coords = yhat[1][idx]\n",
    "    \n",
    "    if yhat[0][idx] > 0.9:\n",
    "        cv2.rectangle(sample_image, \n",
    "                      tuple(np.multiply(sample_coords[:2], [240,240]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [240,240]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "    \n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facedetector.save('facecctv.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1 Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facedetector = load_model('./models/facetracker.h5')\n",
    "facecctv = load_model('facecctv.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 Testing face detection using webcam live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = facedetector.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    \n",
    "    if yhat[0] > 0.5: \n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [1280, 720]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [1280, 720]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [1280, 720]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [1280, 720]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [1280, 720]).astype(int),\n",
    "                                               [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('EyeTrack', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 Testing using dataset static images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('data\\\\footage\\\\0.jpg')\n",
    "plt.imshow(image)\n",
    "\n",
    "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "resized = tf.image.resize(rgb, (120, 120))\n",
    "\n",
    "yhat = facedetector.predict(np.expand_dims(resized/255,0))\n",
    "sample_coords = yhat[1][0]\n",
    "\n",
    "if yhat[0] > 0.5: \n",
    "    # Controls the main rectangle\n",
    "    cv2.rectangle(image, \n",
    "                    tuple(np.multiply(sample_coords[:2], [1280, 720]).astype(int)),\n",
    "                    tuple(np.multiply(sample_coords[2:], [1280, 720]).astype(int)), \n",
    "                        (255,0,0), 2)\n",
    "    # Controls the label rectangle\n",
    "    cv2.rectangle(image, \n",
    "                    tuple(np.add(np.multiply(sample_coords[:2], [1280, 720]).astype(int), \n",
    "                                [0,-30])),\n",
    "                    tuple(np.add(np.multiply(sample_coords[:2], [1280, 720]).astype(int),\n",
    "                                [80,0])), \n",
    "                        (255,0,0), -1)\n",
    "    \n",
    "    # Controls the text rendered\n",
    "    cv2.putText(image, 'face', tuple(np.add(np.multiply(sample_coords[:2], [1280, 720]).astype(int),\n",
    "                                            [0,-5])),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('EyeTrack', image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# load the saved model\n",
    "model = keras.models.load_model(\"facecctv.h5\")\n",
    "\n",
    "# load an image into memory\n",
    "image = cv2.imread(\"data\\\\wider_face_split\\\\WIDER_train\\\\images\\\\0--Parade\\\\0_Parade_marchingband_1_5.jpg\")\n",
    "\n",
    "\n",
    "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "resized = tf.image.resize(rgb, (240, 240))\n",
    "\n",
    "prediction = model.predict(np.expand_dims(resized/255,0))\n",
    "sample_coords = prediction[1][0]\n",
    "\n",
    "#for (x, y, w, h) in prediction:\n",
    "      #cv2.rectangle(image, (x, y), (w, h), (0, 255, 0), 2)\n",
    "\n",
    "# interpret the prediction\n",
    "if prediction[0][0] > 0.5:\n",
    "\n",
    "      print(\"Face detected\")\n",
    "else:\n",
    "      print(\"No face detected\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b503a1eefdb65dd58b69f1b47019840af2c24b3838e9a0722caae24111689e23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
