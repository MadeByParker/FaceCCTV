{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Preprocess the FDDB Dataset\n",
    "\n",
    "Extract the images and annotations from the dataset and preprocess them as necessary. This may involve resizing the images, converting them to grayscale, and normalizing the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script dotenv.exe is installed in 'c:\\Users\\Harry Parker\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script chardetect.exe is installed in 'c:\\Users\\Harry Parker\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awsebcli 3.20.3 requires PyYAML<5.5,>=5.3.1, but you have pyyaml 6.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting roboflow\n",
      "  Downloading roboflow-1.0.5-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 0.0/56.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.2/56.2 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from roboflow) (2.8.2)\n",
      "Collecting cycler==0.10.0\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: six in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from roboflow) (1.14.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from roboflow) (1.26.14)\n",
      "Collecting idna==2.10\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.8/58.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: requests-toolbelt in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from roboflow) (0.10.1)\n",
      "Collecting pyparsing==2.4.7\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "     ---------------------------------------- 0.0/67.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.8/67.8 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: certifi==2022.12.7 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from roboflow) (2022.12.7)\n",
      "Requirement already satisfied: requests in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from roboflow) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from roboflow) (1.24.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from roboflow) (4.64.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from roboflow) (3.7.0)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from roboflow) (9.4.0)\n",
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting PyYAML>=5.3.1\n",
      "  Using cached PyYAML-6.0-cp310-cp310-win_amd64.whl (151 kB)\n",
      "Collecting chardet==4.0.0\n",
      "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "     ---------------------------------------- 0.0/178.7 kB ? eta -:--:--\n",
      "     ------------------------------------- 178.7/178.7 kB 10.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: opencv-python>=4.1.2 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from roboflow) (4.7.0.68)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from roboflow) (1.4.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->roboflow) (1.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->roboflow) (23.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\harry parker\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->roboflow) (4.38.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\harry parker\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->roboflow) (2.0.12)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=633ea2a5a353022d5ad847feca687c8056af913adedc65f50fb45b32bb2964a4\n",
      "  Stored in directory: c:\\users\\harry parker\\appdata\\local\\pip\\cache\\wheels\\8b\\f1\\7f\\5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "Successfully built wget\n",
      "Installing collected packages: wget, PyYAML, python-dotenv, pyparsing, idna, cycler, chardet, roboflow\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 5.1\n",
      "    Uninstalling PyYAML-5.1:\n",
      "      Successfully uninstalled PyYAML-5.1\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.0.9\n",
      "    Uninstalling pyparsing-3.0.9:\n",
      "      Successfully uninstalled pyparsing-3.0.9\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: cycler\n",
      "    Found existing installation: cycler 0.11.0\n",
      "    Uninstalling cycler-0.11.0:\n",
      "      Successfully uninstalled cycler-0.11.0\n",
      "Successfully installed PyYAML-6.0 chardet-4.0.0 cycler-0.10.0 idna-2.10 pyparsing-2.4.7 python-dotenv-1.0.0 roboflow-1.0.5 wget-3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading Dataset Version Zip in Face-detection-1 to voc: 100% [307769650 / 307769650] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to Face-detection-1 in voc:: 100%|██████████| 11552/11552 [00:23<00:00, 482.14it/s]\n"
     ]
    }
   ],
   "source": [
    "%pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"Rx8IIyW6gaJjJza6uCmr\")\n",
    "project = rf.workspace(\"fddb\").project(\"face-detection-40nq0\")\n",
    "dataset = project.version(1).download(\"voc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import img_to_array\n",
    "\n",
    "def convert_data(fddb_path, output_path):\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # Loop over all the fold files in the FDDB dataset\n",
    "    for fold_idx in range(1, 11):\n",
    "        fold_path = os.path.join(fddb_path, 'FDDB-folds', f'FDDB-fold-{fold_idx:02d}.txt')\n",
    "        with open(fold_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for i in range(0, len(lines), 2):\n",
    "                # Read the image and convert to RGB\n",
    "                img_path = os.path.join(fddb_path, lines[i].strip() + '.jpg')\n",
    "                img = cv2.imread(img_path)\n",
    "\n",
    "                # Get the face bounding box coordinates\n",
    "                num_faces = int(lines[i+1])\n",
    "                face_coords = []\n",
    "                for j in range(num_faces):\n",
    "                    face_coord = [int(coord) for coord in lines[i+2+j].split()]\n",
    "                    face_coords.append(face_coord)\n",
    "\n",
    "                # Crop and resize the faces and save them to disk\n",
    "                for face_idx, face_coord in enumerate(face_coords):\n",
    "                    x, y, w, h = face_coord\n",
    "                    face = img[y:y+h, x:x+w]\n",
    "                    face = cv2.resize(face, (224, 224))\n",
    "                    face = img_to_array(face)\n",
    "                    output_file = os.path.join(output_path, f'{fold_idx}_{i//2}_{face_idx}.npy')\n",
    "                    np.save(output_file, face)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '2002/08/26/big/img_265\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m fddb_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/FDDB\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      2\u001b[0m output_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/FDDB/preprocessed/data\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m convert_data(fddb_path, output_path)\n",
      "Cell \u001b[1;32mIn[65], line 23\u001b[0m, in \u001b[0;36mconvert_data\u001b[1;34m(fddb_path, output_path)\u001b[0m\n\u001b[0;32m     20\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(img_path)\n\u001b[0;32m     22\u001b[0m \u001b[39m# Get the face bounding box coordinates\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m num_faces \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(lines[i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     24\u001b[0m face_coords \u001b[39m=\u001b[39m []\n\u001b[0;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_faces):\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '2002/08/26/big/img_265\\n'"
     ]
    }
   ],
   "source": [
    "fddb_path = 'data/FDDB'\n",
    "output_path = 'data/FDDB/preprocessed/data'\n",
    "\n",
    "convert_data(fddb_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Prepare the Training Data\n",
    "\n",
    "Use the annotations in the dataset to create labeled training data.\n",
    "Generate positive and negative samples by extracting image patches that contain faces or do not contain faces, respectively.\n",
    "Split the training data into a training set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_samples.shape: (0,)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load images and preprocess them\n",
    "image_size = (224, 224) # or any other fixed size\n",
    "train_images = []\n",
    "train_annotations = []\n",
    "val_images = []\n",
    "val_annotations = []\n",
    "\n",
    "for image_path in annotations.keys():\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, image_size)\n",
    "    image = image.astype(np.float32) / 255.0 # normalize pixel values to [0, 1]\n",
    "    boxes = annotations[image_path]\n",
    "    \n",
    "    if np.random.random() < 0.8: # 80% of the images for training\n",
    "        train_images.append(image)\n",
    "        train_annotations.append(boxes)\n",
    "    else: # 20% of the images for validation\n",
    "        val_images.append(image)\n",
    "        val_annotations.append(boxes)\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "train_images = np.array(train_images)\n",
    "train_annotations = np.array(train_annotations)\n",
    "val_images = np.array(val_images)\n",
    "val_annotations = np.array(val_annotations)\n",
    "\n",
    "# Generate training and validation samples\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "val_samples = []\n",
    "val_labels = []\n",
    "\n",
    "for i in range(len(train_images)):\n",
    "    sample = train_images[i]\n",
    "    boxes = train_annotations[i]\n",
    "    label = np.zeros((4,))\n",
    "    for box in boxes:\n",
    "        label += np.array([\n",
    "            box[0] / image_size[0],\n",
    "            box[1] / image_size[1],\n",
    "            box[2] / image_size[0],\n",
    "            box[3] / image_size[1],\n",
    "        ])\n",
    "    label /= len(boxes)\n",
    "    train_samples.append(sample)\n",
    "    train_labels.append(label)\n",
    "\n",
    "for i in range(len(val_images)):\n",
    "    sample = val_images[i]\n",
    "    boxes = val_annotations[i]\n",
    "    label = np.zeros((4,))\n",
    "    for box in boxes:\n",
    "        label += np.array([\n",
    "            box[0] / image_size[0],\n",
    "            box[1] / image_size[1],\n",
    "            box[2] / image_size[0],\n",
    "            box[3] / image_size[1],\n",
    "        ])\n",
    "    label /= len(boxes)\n",
    "    val_samples.append(sample)\n",
    "    val_labels.append(label)\n",
    "\n",
    "train_samples = np.array(train_samples)\n",
    "train_labels = np.array(train_labels)\n",
    "val_samples = np.array(val_samples)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "print('train_samples.shape:', train_samples.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123.5833, 85.5495, 1.265839, 269.6934, 161.7812, 1.0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '2002/08/26/big/img_265'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39m# Example usage:\u001b[39;00m\n\u001b[0;32m     63\u001b[0m annotations_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mFDDB\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mFDDB-folds\u001b[39m\u001b[39m\\\u001b[39m\u001b[39moriginal\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mFDDB-fold-01-ellipseList.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 64\u001b[0m image_paths, bboxes \u001b[39m=\u001b[39m load_annotations(annotations_path)\n\u001b[0;32m     65\u001b[0m X, y \u001b[39m=\u001b[39m preprocess_data(image_paths, bboxes, target_size\u001b[39m=\u001b[39m(\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m))\n\u001b[0;32m     66\u001b[0m X_train, X_val, y_train, y_val \u001b[39m=\u001b[39m create_train_val_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[73], line 20\u001b[0m, in \u001b[0;36mload_annotations\u001b[1;34m(annotations_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m     image_paths\u001b[39m.\u001b[39mappend(line\u001b[39m.\u001b[39mstrip())\n\u001b[0;32m     19\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     num_faces \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(line\u001b[39m.\u001b[39;49mstrip())\n\u001b[0;32m     21\u001b[0m     face_bboxes \u001b[39m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_faces):\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '2002/08/26/big/img_265'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def load_annotations(annotations_path):\n",
    "    with open(annotations_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    image_paths = []\n",
    "    bboxes = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if i % 2 == 0:\n",
    "            image_paths.append(line.strip())\n",
    "        else:\n",
    "            num_faces = int(line.strip())\n",
    "            face_bboxes = []\n",
    "            for j in range(num_faces):\n",
    "                face_bbox = [float(x) for x in lines[i+j+1].strip().split()]\n",
    "                print(face_bbox)\n",
    "                # convert from ellipse to bounding box\n",
    "                center_x, center_y, major_axis, minor_axis, angle = face_bbox[0:5]\n",
    "                face_bbox = [center_x - 0.5 * major_axis,\n",
    "                             center_y - 0.5 * minor_axis,\n",
    "                             center_x + 0.5 * major_axis,\n",
    "                             center_y + 0.5 * minor_axis]\n",
    "                face_bboxes.append(face_bbox)\n",
    "            bboxes.append(face_bboxes)\n",
    "    return image_paths, bboxes\n",
    "\n",
    "def preprocess_data(image_paths, bboxes, target_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        img = load_image(image_path)\n",
    "        for bbox in bboxes[i]:\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            x_scale = target_size[0] / img.shape[1]\n",
    "            y_scale = target_size[1] / img.shape[0]\n",
    "            xmin = int(xmin * x_scale)\n",
    "            ymin = int(ymin * y_scale)\n",
    "            xmax = int(xmax * x_scale)\n",
    "            ymax = int(ymax * y_scale)\n",
    "            # crop and resize the face region\n",
    "            face = img[ymin:ymax, xmin:xmax, :]\n",
    "            face = cv2.resize(face, target_size)\n",
    "            X.append(face)\n",
    "            y.append([xmin, ymin, xmax, ymax])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "def create_train_val_split(X, y, test_size):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "annotations_path = 'data\\FDDB\\FDDB-folds\\original\\FDDB-fold-01-ellipseList.txt'\n",
    "image_paths, bboxes = load_annotations(annotations_path)\n",
    "X, y = preprocess_data(image_paths, bboxes, target_size=(224, 224))\n",
    "X_train, X_val, y_train, y_val = create_train_val_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Define the Model Architecture\n",
    "\n",
    "Choose a suitable model architecture for face detection. You can use pre-trained models such as YOLO, RetinaNet, or SSD or build your own custom model.\n",
    "Define the model architecture using a deep learning framework such as TensorFlow or Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras.applications.resnet import ResNet50\n",
    "\n",
    "def build_model():\n",
    "    # Define the input shape\n",
    "    input_shape = (None, None, 3)\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # Define the backbone network (ResNet50 in this example)\n",
    "    backbone = ResNet50(weights='imagenet', include_top=False)\n",
    "    backbone_output = backbone(input_layer)\n",
    "    \n",
    "    # Define the detection head\n",
    "    detection_head = Conv2D(64, (3, 3), activation='relu', padding='same')(backbone_output)\n",
    "    detection_head = MaxPooling2D((2, 2))(detection_head)\n",
    "    detection_head = Conv2D(128, (3, 3), activation='relu', padding='same')(detection_head)\n",
    "    detection_head = MaxPooling2D((2, 2))(detection_head)\n",
    "    detection_head = GlobalAveragePooling2D()(detection_head)\n",
    "    detection_head = Dense(256, activation='relu')(detection_head)\n",
    "    detection_head = Dense(4, activation='sigmoid')(detection_head)\n",
    "    \n",
    "    # Define the model\n",
    "    model = Model(inputs=input_layer, outputs=detection_head)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train the Model\n",
    "\n",
    "Train the model on the training data using a suitable optimizer and loss function.\n",
    "Monitor the training process by plotting the loss and accuracy during each epoch.\n",
    "Evaluate the model on the validation set to ensure that it is not overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, None, None, 2048)  23587712  \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, None, None, 64)    1179712   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, None, None, 64)   0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, None, None, 128)  0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,875,332\n",
      "Trainable params: 24,822,212\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(val_samples\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(val_labels\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 18\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_samples, train_labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(val_samples, val_labels))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py:1662\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1660\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   1661\u001b[0m \u001b[39mif\u001b[39;00m logs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1662\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1663\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnexpected result of `train_function` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1664\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(Empty logs). Please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1665\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1666\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1667\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minformation of where went wrong, or file a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1668\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39missue/bug to `tf.keras`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1669\u001b[0m     )\n\u001b[0;32m   1670\u001b[0m \u001b[39m# Override with model metrics instead of last step logs\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_and_get_metrics_result(logs)\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "# Generate training data\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "print(train_samples.shape)\n",
    "print(train_labels.shape)\n",
    "print(val_samples.shape)\n",
    "print(val_labels.shape)\n",
    "\n",
    "history = model.fit(train_samples, train_labels, epochs=10, batch_size=32, validation_data=(val_samples, val_labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss and accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Plot the loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history)\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plot the accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['acc'], label='train')\n",
    "plt.plot(history.history['val_acc'], label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Load the model\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('best_model.h5')\n",
    "\n",
    "\n",
    "# Generate test data\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "for annotation in annotations:\n",
    "    filename, x1, y1, x2, y2 = annotation\n",
    "    image = cv2.imread(filename)\n",
    "    if image is None:\n",
    "        continue\n",
    "    size = np.random.randint(12, min(image.shape[:2]))\n",
    "    x = np.random.randint(0, image.shape[1]-size)\n",
    "    y = np.random.randint(0, image.shape[0]-size)\n",
    "    sample = image[y:y+size, x:x+size]\n",
    "    sample = cv2.resize(sample, (64, 64))\n",
    "    test_samples.append(sample)\n",
    "    test_labels.append([1, x1-x, y1-y, x2-x, y2-y])\n",
    "test_samples = np.array(test_samples)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(test_samples, test_labels)\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_samples)\n",
    "\n",
    "\n",
    "# Visualize the predictions\n",
    "for i in range(10):\n",
    "    sample = test_samples[i]\n",
    "    prediction = predictions[i]\n",
    "    x1 = int(prediction[1])\n",
    "    y1 = int(prediction[2])\n",
    "    x2 = int(prediction[3])\n",
    "    y2 = int(prediction[4])\n",
    "    cv2.rectangle(sample, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    plt.imshow(sample)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
