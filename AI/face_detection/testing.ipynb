{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_wider_face_annotations(data_dir):\n",
    "    \"\"\"\n",
    "    Load WIDER face dataset annotations from text files into a pandas DataFrame.\n",
    "    Args:\n",
    "        data_dir: str, path to the directory containing the dataset files.\n",
    "    Returns:\n",
    "        annotations: pandas DataFrame containing the annotations for the WIDER face dataset.\n",
    "    \"\"\"\n",
    "    # Define the paths to the annotation files\n",
    "    train_file = os.path.join(data_dir, 'wider_face_train_bbx_gt.txt')\n",
    "    val_file = os.path.join(data_dir, 'wider_face_val_bbx_gt.txt')\n",
    "    test_file = os.path.join(data_dir, 'wider_face_test_filelist.txt')\n",
    "\n",
    "    # Load the training annotations\n",
    "    train_annotations = pd.read_csv(train_file, sep='\\t', header=None,\n",
    "                                    names=['filename', 'num_faces', 'x1', 'y1', 'width', 'height', 'blur', 'expression', 'illumination', 'invalid', 'occlusion', 'pose'])\n",
    "\n",
    "    # Drop the columns we don't need\n",
    "    train_annotations = train_annotations.drop(['num_faces', 'blur', 'expression', 'illumination', 'invalid', 'occlusion', 'pose'], axis=1)\n",
    "\n",
    "    # Add the full path to the image filename\n",
    "    train_annotations['filename'] = train_annotations['filename'].apply(lambda x: os.path.join(data_dir, 'WIDER_train/images', x))\n",
    "\n",
    "    # Take the first 1400 images\n",
    "    train_annotations = train_annotations[:1400]\n",
    "\n",
    "    # Load the validation annotations\n",
    "    val_annotations = pd.read_csv(val_file, sep='\\t', header=None,\n",
    "                                  names=['filename', 'num_faces', 'x1', 'y1', 'width', 'height', 'blur', 'expression', 'illumination', 'invalid', 'occlusion', 'pose'])\n",
    "\n",
    "    # Drop the columns we don't need\n",
    "    val_annotations = val_annotations.drop(['num_faces', 'blur', 'expression', 'illumination', 'invalid', 'occlusion', 'pose'], axis=1)\n",
    "\n",
    "    # Add the full path to the image filename\n",
    "    val_annotations['filename'] = val_annotations['filename'].apply(lambda x: os.path.join(data_dir, 'WIDER_val/images', x))\n",
    "\n",
    "    # Take the first 200 images\n",
    "    val_annotations = val_annotations[:200]\n",
    "\n",
    "    # Load the test annotations\n",
    "    test_annotations = pd.read_csv(test_file, sep=' ', header=None, names=['filename'])\n",
    "\n",
    "    # Add the full path to the image filename\n",
    "    test_annotations['filename'] = test_annotations['filename'].apply(lambda x: os.path.join(data_dir, 'WIDER_test/images', x + '.jpg'))\n",
    "\n",
    "    # Take the first 400 images\n",
    "    test_annotations = test_annotations[:400]\n",
    "\n",
    "    # Combine the training, validation, and test annotations into a single DataFrame\n",
    "    annotations = pd.concat([train_annotations, val_annotations, test_annotations], ignore_index=True)\n",
    "\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/WIDERFace/wider_face_split'\n",
    "annotations = load_wider_face_annotations(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_custom_datasets(annotations, train_dir, test_dir, validation_dir, train_size=1400, test_size=400, val_size=200):\n",
    "    \"\"\"\n",
    "    Load custom train, test, and validation datasets from the WIDER Face dataset annotations and images.\n",
    "    Args:\n",
    "        annotations: str, the directory containing the WIDER Face dataset annotations.\n",
    "        image_dir: str, the directory containing the WIDER Face dataset images.\n",
    "        train_size: int, the number of images to use for training.\n",
    "        test_size: int, the number of images to use for testing.\n",
    "        val_size: int, the number of images to use for validation.\n",
    "    Returns:\n",
    "        train_dataset: tuple, (list of numpy arrays, list of numpy arrays), the train dataset images and labels.\n",
    "        test_dataset: tuple, (list of numpy arrays, list of numpy arrays), the test dataset images and labels.\n",
    "        val_dataset: tuple, (list of numpy arrays, list of numpy arrays), the validation dataset images and labels.\n",
    "    \"\"\"\n",
    " # Load the first train_size images for the train dataset.\n",
    "    train_annotation_path = os.path.join(annotations, 'wider_face_train_bbx_gt.txt')\n",
    "    with open(train_annotation_path) as f:\n",
    "        train_lines = f.readlines()\n",
    "    train_lines = train_lines[:train_size * 2] # Multiply by 2 since each image has 2 lines of annotations\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    for i in range(0, len(train_lines), 2):\n",
    "        filepath = os.path.join(train_dir, train_lines[i].strip())\n",
    "        image = np.array(Image.open(filepath))\n",
    "        num_boxes = int(train_lines[i + 1])\n",
    "        boxes = []\n",
    "        for j in range(num_boxes):\n",
    "            box_coords = np.array(train_lines[i + 2 + j].strip().split(), dtype=np.float32)\n",
    "            boxes.append(box_coords)\n",
    "        train_images.append(image)\n",
    "        train_labels.append(np.array(boxes))\n",
    "\n",
    "    # Load the first test_size images for the test dataset.\n",
    "    test_annotation_path = os.path.join(annotations, 'wider_face_test_filelist.txt')\n",
    "    with open(test_annotation_path) as f:\n",
    "        test_lines = f.readlines()\n",
    "    test_lines = test_lines[:test_size * 2] # Multiply by 2 since each image has 2 lines of annotations\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    for i in range(0, len(test_lines), 2):\n",
    "        filepath = os.path.join(test_dir, test_lines[i].strip())\n",
    "        image = np.array(Image.open(filepath))\n",
    "        test_images.append(image)\n",
    "        test_labels.append(np.array([]))\n",
    "\n",
    "    # Load the first val_size images for the validation dataset.\n",
    "    val_annotation_path = os.path.join(annotations, 'wider_face_val_bbx_gt.txt')\n",
    "    with open(val_annotation_path) as f:\n",
    "        val_lines = f.readlines()\n",
    "    val_lines = val_lines[:val_size * 2] # Multiply by 2 since each image has 2 lines of annotations\n",
    "    val_images = []\n",
    "    val_labels = []\n",
    "    for i in range(0, len(val_lines), 2):\n",
    "        filepath = os.path.join(validation_dir, val_lines[i].strip())\n",
    "        image = np.array(Image.open(filepath))\n",
    "        num_boxes = int(val_lines[i + 1])\n",
    "        boxes = []\n",
    "        for j in range(num_boxes):\n",
    "            box_coords = np.array(val_lines[i + 2 + j].strip().split(), dtype=np.float32)\n",
    "            boxes.append(box_coords)\n",
    "        val_images.append(image)\n",
    "        val_labels.append(np.array(boxes))\n",
    "\n",
    "    train_dataset = (train_images, train_labels)\n",
    "    test_dataset = (test_images, test_labels)\n",
    "    val_dataset = (val_images, val_labels)\n",
    "\n",
    "    return train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/WIDERFace/WIDER_train\\\\449 330 122 149 0 0 0 0 0 0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m validation_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/WIDERFace/WIDER_val\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      4\u001b[0m annotation_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/WIDERFace/wider_face_split\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m train_dataset, test_dataset, val_dataset \u001b[39m=\u001b[39m load_custom_datasets(annotation_dir, train_dir, test_dir, validation_dir)\n",
      "Cell \u001b[1;32mIn[13], line 29\u001b[0m, in \u001b[0;36mload_custom_datasets\u001b[1;34m(annotations, train_dir, test_dir, validation_dir, train_size, test_size, val_size)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(train_lines), \u001b[39m2\u001b[39m):\n\u001b[0;32m     28\u001b[0m     filepath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(train_dir, train_lines[i]\u001b[39m.\u001b[39mstrip())\n\u001b[1;32m---> 29\u001b[0m     image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(Image\u001b[39m.\u001b[39;49mopen(filepath))\n\u001b[0;32m     30\u001b[0m     num_boxes \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(train_lines[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m])\n\u001b[0;32m     31\u001b[0m     boxes \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\PIL\\Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3224\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3226\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3227\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   3228\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3230\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/WIDERFace/WIDER_train\\\\449 330 122 149 0 0 0 0 0 0'"
     ]
    }
   ],
   "source": [
    "train_dir = 'data/WIDERFace/WIDER_train'\n",
    "test_dir = 'data/WIDERFace/WIDER_test'\n",
    "validation_dir = 'data/WIDERFace/WIDER_val'\n",
    "annotation_dir = 'data/WIDERFace/wider_face_split'\n",
    "train_dataset, test_dataset, val_dataset = load_custom_datasets(annotation_dir, train_dir, test_dir, validation_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"conv2_class\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (None, 100, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m localization_layers \u001b[39m=\u001b[39m []\n\u001b[0;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_feature_maps):\n\u001b[1;32m---> 24\u001b[0m     x \u001b[39m=\u001b[39m Conv2D(num_anchors \u001b[39m*\u001b[39;49m num_classes, (\u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m), padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msame\u001b[39;49m\u001b[39m'\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mconv\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m_class\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m))(x)\n\u001b[0;32m     25\u001b[0m     x \u001b[39m=\u001b[39m Reshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, num_classes))(x)\n\u001b[0;32m     26\u001b[0m     classification_layers\u001b[39m.\u001b[39mappend(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\input_spec.py:250\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    248\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n\u001b[0;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m ndim \u001b[39m<\u001b[39m spec\u001b[39m.\u001b[39mmin_ndim:\n\u001b[1;32m--> 250\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    251\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected min_ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mmin_ndim\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m         )\n\u001b[0;32m    257\u001b[0m \u001b[39m# Check dtype.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"conv2_class\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (None, 100, 1)"
     ]
    }
   ],
   "source": [
    "input_shape = (300, 300, 3)\n",
    "num_classes = 1\n",
    "num_anchors = 4\n",
    "num_feature_maps = 6\n",
    "aspect_ratios = [[1, 2, 0.5], [1, 2, 3, 0.5, 0.3333], [1, 2, 3, 0.5, 0.3333], [1, 2, 3, 0.5, 0.3333], [1, 2, 0.5], [1, 2, 0.5]]\n",
    "scales = [0.1, 0.2, 0.375, 0.55, 0.725, 0.9]\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "x = inputs\n",
    "\n",
    "for i in range(num_feature_maps):\n",
    "    x = Conv2D(256, (1, 1), padding='same', name='conv{}'.format(i+1))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = ZeroPadding2D()(x)\n",
    "    x = Conv2D(512, (3, 3), strides=(2, 2), name='conv{}_2'.format(i+1))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "classification_layers = []\n",
    "localization_layers = []\n",
    "\n",
    "for i in range(num_feature_maps):\n",
    "    x = Conv2D(num_anchors * num_classes, (3, 3), padding='same', name='conv{}_class'.format(i+1))(x)\n",
    "    x = Reshape((-1, num_classes))(x)\n",
    "    classification_layers.append(x)\n",
    "\n",
    "for i in range(num_feature_maps):\n",
    "    x = Conv2D(num_anchors * 4, (3, 3), padding='same', name='conv{}_box'.format(i+1))(x)\n",
    "    x = Reshape((-1, 4))(x)\n",
    "    localization_layers.append(x)\n",
    "\n",
    "classification_output = Concatenate(axis=1, name='classification')(classification_layers)\n",
    "localization_output = Concatenate(axis=1, name='localization')(localization_layers)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[classification_output, localization_output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.backend import epsilon\n",
    "\n",
    "def smooth_l1(y_true, y_pred):\n",
    "    absolute_loss = tf.abs(y_true - y_pred)\n",
    "    square_loss = 0.5 * (y_true - y_pred)**2\n",
    "    loss = tf.where(tf.less(absolute_loss, 1.0), square_loss, absolute_loss - 0.5)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def total_loss(y_true, y_pred):\n",
    "    classification_loss = binary_crossentropy(y_true[:,:,:1], y_pred[:,:,:1])\n",
    "    localization_loss = smooth_l1(y_true[:,:,1:], y_pred[:,:,1:])\n",
    "    regularization_loss = tf.reduce_sum(tf.abs(y_pred))\n",
    "    return classification_loss + localization_loss + 0.001 * regularization_loss\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=total_loss, metrics=[smooth_l1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     annotations \u001b[39m=\u001b[39m [(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_path, \u001b[39m'\u001b[39m\u001b[39mWIDER_train\u001b[39m\u001b[39m'\u001b[39m, line[\u001b[39m0\u001b[39m]), \u001b[39mint\u001b[39m(line[\u001b[39m1\u001b[39m]), [\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mint\u001b[39m, box\u001b[39m.\u001b[39msplit())) \u001b[39mfor\u001b[39;00m box \u001b[39min\u001b[39;00m line[\u001b[39m2\u001b[39m:]]) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m annotations]\n\u001b[0;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m annotations\n\u001b[1;32m---> 19\u001b[0m train_annotations \u001b[39m=\u001b[39m load_annotations(train_file)\n\u001b[0;32m     20\u001b[0m val_annotations \u001b[39m=\u001b[39m load_annotations(val_file)\n\u001b[0;32m     22\u001b[0m \u001b[39m# Define the classes\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 16\u001b[0m, in \u001b[0;36mload_annotations\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     14\u001b[0m     annotations \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[0;32m     15\u001b[0m annotations \u001b[39m=\u001b[39m [line\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit() \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m annotations]\n\u001b[1;32m---> 16\u001b[0m annotations \u001b[39m=\u001b[39m [(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_path, \u001b[39m'\u001b[39m\u001b[39mWIDER_train\u001b[39m\u001b[39m'\u001b[39m, line[\u001b[39m0\u001b[39m]), \u001b[39mint\u001b[39m(line[\u001b[39m1\u001b[39m]), [\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mint\u001b[39m, box\u001b[39m.\u001b[39msplit())) \u001b[39mfor\u001b[39;00m box \u001b[39min\u001b[39;00m line[\u001b[39m2\u001b[39m:]]) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m annotations]\n\u001b[0;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m annotations\n",
      "Cell \u001b[1;32mIn[22], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m     annotations \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[0;32m     15\u001b[0m annotations \u001b[39m=\u001b[39m [line\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit() \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m annotations]\n\u001b[1;32m---> 16\u001b[0m annotations \u001b[39m=\u001b[39m [(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_path, \u001b[39m'\u001b[39m\u001b[39mWIDER_train\u001b[39m\u001b[39m'\u001b[39m, line[\u001b[39m0\u001b[39m]), \u001b[39mint\u001b[39m(line[\u001b[39m1\u001b[39;49m]), [\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mint\u001b[39m, box\u001b[39m.\u001b[39msplit())) \u001b[39mfor\u001b[39;00m box \u001b[39min\u001b[39;00m line[\u001b[39m2\u001b[39m:]]) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m annotations]\n\u001b[0;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m annotations\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the WIDER Face dataset\n",
    "dataset_path = 'data/WIDERFace'\n",
    "\n",
    "# Load the annotations\n",
    "train_file = os.path.join(dataset_path, 'wider_face_split', 'wider_face_train_bbx_gt.txt')\n",
    "val_file = os.path.join(dataset_path, 'wider_face_split', 'wider_face_val_bbx_gt.txt')\n",
    "\n",
    "def load_annotations(file_path):\n",
    "    with open(file_path) as f:\n",
    "        annotations = f.readlines()\n",
    "    annotations = [line.strip().split() for line in annotations]\n",
    "    annotations = [(os.path.join(dataset_path, 'WIDER_train', line[0]), int(line[1]), [list(map(int, box.split())) for box in line[2:]]) for line in annotations]\n",
    "    return annotations\n",
    "\n",
    "train_annotations = load_annotations(train_file)\n",
    "val_annotations = load_annotations(val_file)\n",
    "\n",
    "# Define the classes\n",
    "classes = ['face']\n",
    "\n",
    "# Define the input size\n",
    "input_shape = (300, 300)\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_data(image_path, annotations):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, input_shape)\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for annotation in annotations:\n",
    "        box = np.array(annotation[:4])\n",
    "        box = np.clip(box, 0, input_shape[0])\n",
    "        box[2:] = box[2:] - box[:2]\n",
    "        boxes.append(box)\n",
    "        labels.append(classes.index('face'))\n",
    "    return image, np.array(boxes), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the backbone network (ResNet50)\n",
    "backbone = tf.keras.applications.ResNet50(include_top=False, input_shape=(300, 300, 3))\n",
    "\n",
    "# Define the SSD model\n",
    "num_classes = 1\n",
    "input_shape = (300, 300, 3)\n",
    "ssd_model = tf.keras.models.Sequential(name='ssd')\n",
    "ssd_model.add(backbone)\n",
    "ssd_model.add(tf.keras.layers.Conv2D(1024, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "ssd_model.add(tf.keras.layers.Conv2D(1024, kernel_size=(1, 1), padding='same', activation='relu'))\n",
    "ssd_model.add(tf.keras.layers.Conv2D(4*num_classes, kernel_size=(1, 1), padding='same', activation=None))\n",
    "ssd_model.add(tf.keras.layers.Reshape((-1, 4)))\n",
    "ssd_model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "ssd_loss = tf.keras.losses.Huber()\n",
    "ssd_optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "ssd_model.compile(optimizer=ssd_optimizer, loss=ssd_loss)\n",
    "\n",
    "# Prepare the WIDER Face dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "train_dataset = train_dataset.map(preprocess_data)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000)\n",
    "train_dataset = train_dataset.batch(batch_size=32)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(val_data)\n",
    "val_dataset = val_dataset.map(preprocess_data)\n",
    "val_dataset = val_dataset.batch(batch_size=32)\n",
    "\n",
    "# Train the model\n",
    "epochs = 50\n",
    "steps_per_epoch = len(train_data) // batch_size\n",
    "validation_steps = len(val_data) // batch_size\n",
    "\n",
    "ssd_model.fit(train_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=val_dataset, validation_steps=validation_steps)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss = ssd_model.evaluate(val_dataset)\n",
    "\n",
    "# Compute the average precision (AP) for different IoU thresholds\n",
    "iou_thresholds = [0.5, 0.7, 0.9]\n",
    "average_precisions = ssd_utils.compute_average_precisions(val_dataset, ssd_model, iou_thresholds=iou_thresholds)\n",
    "\n",
    "# Use the trained model to detect faces in new images\n",
    "test_image = cv2.imread('test_image.jpg')\n",
    "test_image = cv2.resize(test_image, input_shape[:2])\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "detections = ssd_model.predict(test_image)\n",
    "\n",
    "# Post-process the output bounding box predictions to remove duplicates and non-maximum suppression\n",
    "boxes, scores = ssd_utils.decode_detections(detections, confidence_thresh=0.5, iou_threshold=0.45, top_k=200)\n",
    "boxes, scores = ssd_utils.non_maximum_suppression(boxes, scores, max_output_size=50, iou_threshold=0.45)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
